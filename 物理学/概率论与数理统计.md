# 概率论的基本概念

概率：又称或然率、几率，是表示某种情况(事件)出现的可能性大小的一种数量指标，它介于$0$与$1$之间.

事件：概率论中，事件指对某种(或某些)情况的陈述，它可能发生，也可能不发生，发生与否，要到有关的“试验”有了结果以后才能知晓.

“事件”的一般含义：

(1)有一个明确界定的试验

(2)这个试验的全部结果，是在试验前就明确的.

(3)我们有一个明确的陈述，这个陈述界定了试验的全部可能结果中一个确定的部分.这个陈述，或者说一个确定的部分，就叫做一个确定的部分.

基本事件：在概率论上，有时把单一的试验结果称为一个“基本事件”.这样，一个或一些基本事件并在一起，就构成一个事件，而基本事件本身也是事件.

随机事件:若某个事件在某次实验中是否发生取决于机遇，则称其为“随机事件”.随机事件的极端情况是“必然事件”和“不可能事件”.

必然事件:在试验中必然发生的事件称为“必然事件”.

不可能事件:在试验中不可能发生的事件称为“不可能事件”.

可以把必然事件和不可能事件分别等同于概率为$1$和概率为$0$的事件.从严格的理论角度而言，这二者有所区别，但这种区别并无实际的重要性.

$$
\begin{aligned}
&交换律：A\cup B=B\cup A;~~A\cap B=B\cap A \\
&结合律：A\cup(B\cup C)=(A\cup B)\cup C;~~ A\cap(B\cap C)=(A\cap B)\cap C \\
&分配律：A\cup(B\cap C)=(A\cup B)\cap(A\cup C);~~A\cap(B\cup C)=(A\cap B)\cup(B\cap C) \\
&德摩根律：\overline{A\cup B}=\overline{A}\cap\overline{B};~~\overline{A\cap B}=\overline{A}\cup\overline{B}
\end{aligned}
$$

概率的定义：

设 $E$ 是随机试验，$S$ 是它的样本空间. 对于 $E$ 的每一事件 $A$ 赋予一个实数，记为 $P(A)$，称为事件 $A$ 的概率，若集合函数 $P(\cdot)$ 满足以下条件：

$$
\begin{aligned}
&1.非负性：对于每一个事件A,有:P(A)\geqslant 0 \\
&2.规范性：对于必然事件S:有P(S)=1 \\
&3.可列可加性：设A_1,A_2,\cdots是两两互不相容的事件，即对于A_iA_j=\varnothing,i\ne j,i,j=1,2,\cdots,有：
P(A_1\cup A_2\cup\cdots)=P(A_1)+P(A_2)+\cdots
\end{aligned}
$$

概率的性质：

性质一：$P(\varnothing)=0$

性质二：若 $A_1,A_2,A_n$ 是两两互不相容的事件，则有：

$$
P(A_1\cup A_2\cup\cdots\cup A_n)
=P(A_1)+P(A_2)+\cdots+P(A_n)
$$

性质三：设 $A,B$ 是两个事件，若$A\sub B $，则有：

$$
P(B-A)
=P(B)-P(A)
$$

性质四：对于任一事件 $A$，有：

$$
P(A)\leqslant 1
$$

性质五：对于任一事件 $A$，有：

$$
P(\bar{A})
=1-P(A)
$$

性质六：对于任意两事件 $A,B$，有：

$$
P(A\cup B)
=P(A)+P(B)-P(A\cap B)
$$

等可能概型/古典概型:

假定某个试验有有限个可能的结果$e_1,e_2,\cdots,e_N$.假定从该试验的条件及实施方法上去分析，我们找不到任何理由认为其中某一结果，例如$e_i$,比任一其他结果，例如$e_j(i\ne j)$,更具有优势(即更容易发生)
，于是我们只好认为，所有结果$e_1,e_2\cdots,e_N$在试验中有同等可能的出现机会，即$\frac{1}{N}$的出现机会.我们常常把这样的试验结果称为“等可能的”.

古典概率：

设一个试验有$N$个等可能的结果,而事件$E$恰包含其中的$M$个结果，则事件$E$的概率，记为$P(E)$，定义为：

$$
P(E)=\frac{M}{N}
$$

例(cxr例1.1)

例(cxr例1.2,几何概率)

频率：

反复做大量试验，

概率的统计定义：

把事件$E$的概率定义为具有如下性质的一个数$p$:当把试验重复，$E$的频率在$p$的附近摆动，且当重复次数增大时，这个摆动愈来愈小.或者干脆说，概率就是当试验次数无限增大时频率的极限

概率的公理化定义：

排列：

$$
P^n_r
=n(n-1)(n-2)\cdots(n-r+1)
=\frac{n!}{(n-r)!}
$$

组合：

$$
\tbinom{n}{r}
=\frac{P^n_r}{r!}
=\frac{n(n-1)(n-2)\cdots(n-r+1)}{r!}
=\frac{n!}{r!(n-r)!}
$$

古典概率计算：

例(cxr例2.1)：

例(cxr例2.2)：

$n$双相异的鞋公$2n$只，随机地分成$n$堆，每堆$2$只.问“各堆都自成一双鞋”这个事件$E$的概率是多少？

答案：$P(E)=\frac{1}{(2n-1)!!},$其中，$"!!"$这个符号对**奇自然数**地定义为：$a!!=1\cdot3\cdot 5\cdots a$

解法一：

$2n$双相异的鞋，随机分成$n$堆，每堆$2$只，计较排列顺序的话，共有排法：

$$
N
=\tbinom{2n}{2}\tbinom{2n-2}{2}\cdots\tbinom{2}{2}
=\frac{(2n)!}{2^n}
$$

$2n$双相异的鞋，随机分成$n$堆，每堆$2$只，且恰好各堆都自成一双鞋，计较排列顺序的话，共有排法：

$$
M=n!
$$

于是：

$$
P(E)=\frac{M}{N}=\frac{n!2^n}{(2n)!}=\frac{1}{(2n-1)!!}
$$

解法二：

同样有：

$$
N
=\tbinom{2n}{2}\tbinom{2n-2}{2}\cdots\tbinom{2}{2}
=\frac{(2n)!}{2^n}
$$

不同的是$M$的计算：

$$
M=(2n)\cdot(2n-2)\cdots(2)=n!2^n
$$

于是：

$$
P(E)=\frac{M}{N}=\frac{1}{(2n-1)!!}
$$


例(cxr例2.3):

$n$个男孩，$m$个女孩$(m\leqslant n+1)$随机地排成一列.问“任意两个女孩都不相邻”这个事件$E$的概率是多少？

答案：$P(E)=\frac{\tbinom{n+1}{m}}{\tbinom{n+m}{m}}$

解法(插空法)：

所有人随机地排成一列，总共有排法：

$$
N=(n+m)!
$$

所有人随机地排成一列，且任意两个女孩不相邻的排法：

$$
M=n!\tbinom{n+1}{m}m!
$$

于是：

$$
P(E)
=\frac{M}{N}
=\frac{n!\tbinom{n+1}{m}m!}{(n+m)!}
=\frac{\tbinom{n+1}{m}}{\tbinom{n+m}{m}}
$$

例(cxr例2.4)：

答案：$P=\frac{\tbinom{2n-m}{n}}{2^{2n-m}}$

解法一：

例(cxr例2.5):

答案：$P=\frac{17!21!}{17^{21}2!^3 3!^4 4!5!6!} $

例(cxr例2.6):

答案：$P=1-\frac{1}{2^{n-1}} $

### 事件的运算、条件概率与独立性

在实用上和理论上，下述情况常见：问题中有许多比较简单的事件，其概率易于算出，或是有了理论上的假定值，或是根据以往的经验已对其值作了充分精确的估计.而我们感兴趣的是一个复杂的事件$E,$它通过种种关系与上述简单
事件联系起来，这时我们想设法利用这种联系，以便从这些简单事件的概率去算出$E$的概率.正如在微积分中，直接利用定义可算出若干简单函数的导数，但利用导数所满足的法则，可据此算出很复杂的函数的导数.

事件的蕴含、包含及相等：

在**同一试验下**的两个事件$A$和$B$，如果当$A$发生时$B$必发生，则称$A$蕴含$B$，或者说$B$包含$A$，记为$A\subset B$.若$A,B$相互蕴含，即:$A\subset B$且$B\subset A$，则称$A,B$两事件相等，记为：$A=B$

拿“事件是试验的一些结果”这个观点来看，若$A$蕴含$B$，那只能是：$A$中的试验结果必在$B$中，即$B$这个集合(作为试验结果的集合)要大一些，“包含”一词由此而来.实际含义是：若$A \subset B$(也写为$B \supset A$)，则$A$和$B$相比，更难发生一些，因而其概率就必然小于或至多等于$B$的概率.“两事件$A,B$”相等，无非是说，$A,B$由完全同一的一些试验结果构成，它不过是同一事件表面上看来不同的两个说法而已.

证明两事件相等的一般方法是：先设事件$A$发生，由此推出事件$B$发生；再反过来，由假定$B$发生推出$A$发生.

事件的互斥和对立：

互斥事件

若两事件$A,B$不能在同一次试验中都发生(但可以都不发生)，则称它们是互斥的.如果一些事件中任意两个都互斥，则称这些事件是两两互斥的，或简称互斥的.

从“事件是一些试验结果所构成的”这个观点看，互斥事件无非是说，构成这两个事件各自的试验结果中不能有公共的.

对立事件：

互斥事件的一个重要情况是“对立事件”.若$A$为一事件，则事件：

$$
B=\{A不发生 \}
$$

称为$A$的对立事件，多记为$\bar{A} $(读作$A~~ bar$，也记为$A^c$)

对立事件也常称为“补事件”.若两个事件互为对立事件，则它们的和是必然事件，它们的交集是空集.

事件的和(或称并)：

设有两个事件$A$和$B$，**定义**一个新事件$C$如下：

$$
C=\{A发生，或B发生 \}=\{A,B至少发生一个 \}
$$

所谓**定义一个事件**，就是指出它何时发生，何时不发生.在上面的定义下，$C$在何时发生呢？只要$A$发生或者$B$发生(或二者同时发生也可以)，就算是$C$发生了，不然(即$A,B$都不发生)则算作$C$不发生.这样定义的事件$C$称为事件$A$与事件$B$的和，记为：

$$
C=A+B
$$

有时也记为$C=A\bigcup B$，不过本书采用$C=A+B$

两事件的和，即把构成各自事件的那些试验结果并在一起所构成的事件

这样，若$C=A+B$，则$A,B$都蕴含$C$，$C$包含$A$也包含$B$.加过相加，事件变得更大了(含有了更多的试验结果)，因而更容易发生了.

事件的和很自然地推广到多个事件的情形，

概率的加法定律：

定理：

若干互斥事件之和的概率，等于各事件的概率之和，即：若$A_1,A_2,\cdots,$两两互斥，则有：

$$
P(A_1+A_2+\cdots)=P(A_1)+P(A_2)+\cdots
$$

上述定理中，事件个数可以是有限的，也可以是无限的.结论成立的重要条件(或者说前提)是**各事件必须两两互斥**.

证明(概率的古典定义下)：

加法定理的一个重要推论：

以$\bar{A} $表示$A$的对立事件，则：

$$
P(\bar{A})=1-P(A)
$$

事件的积(或称交)、事件的差

设有两个事件$A,B$，则如下定义的事件$C$:

$$
C={A,B都发生}
$$

称为两事件$A,B$的积或乘积，并记为$AB$.一般地，事件$A,B$各是一些试验结果的集合，而$AB$则由同属于这两个集合的哪些试验结果组成，即这两个集合的交叉.按积的定义，两个事件$A,B$互斥，等于说$AB$是不可能事件.

多个事件的积的定义

两个事件$A,B$之差，记为$A-B$，定义为：

$$
A-B=\{A发生,B不发生 \}
$$

一般地，$A-B$就是从构成$A$的那些试验中，去掉在$B$内的那一些

$$
A-B=A\bar{B}
$$

运算规律：

上面引进了和、积、差等运算，下面是成立的运算规则：

$$
A+B=B+A \\
AB=BA \\
A(B-C)=AB-AC
$$

第三行证明如下：

条件概率：

一般来讲，条件概率就是附加在一定条件之下所计算的概率.从广义的意义上说，任何概率都是条件概率，因为我们是在一定的试验之下去考虑事件的概率的，而试验即规定有条件.在概率论中，决定试验的那些基础条件被看作已定不变的.如果不再加入其他条件或假定，则算出的概率就叫做“无条件概率”，就是通常所说的概率.当说到“条件概率”时，总是指另外附加的条件，其形式总可归结为“已知某事件发生了”.

在古典概率的模式下分析一般的情况：

设一试验有$N$个等可能的结果，事件$A,B$分别包含其中的$M_1$个和$M_2$个结果，它们有$M_{12}$是公共的，这就是事件$AB$所包含的试验结果数.若已给定$B$发生，则我们的考虑由起先的$N$个可能结果局限到现在的$M_2$个，其中只有$M_{12}$个试验结果使事件$A$发生，故一个合理的条件概率定义，应把$P(A|B)$取为$\frac{M_{12}}{M_2}$.但：

$$
\frac{M_{12}}{M_2}
=\frac{M_{12}/N}{M_2 /N}=\frac{P(AB)}{P(B)}
$$

由此得出如下的一般定义：

设有两个事件$A,B$，而$P(B)\ne 0$.则“在给定$B$发生的条件下$A$的条件概率”，记为$P(A|B)$，定义为：

$$
P(A|B)=\frac{P(AB)}{P(B)}
$$

上面是条件概率的一般定义，但在计算条件概率时，并不一定要用它.有时，直接从加入条件后改变了的情况去算，更为方便.

例(cxr例3.1):

答案：$\frac{5}{18} $

事件的独立性，概率乘法定理：

若$P(A)=P(A|B),$则$B$的发生与否对$A$发生的可能性毫无影响.这时，在概率论上就称$A,B$两事件独立.

定义：称两个事件$A,B$独立，若它们满足以下条件：

$$
P(A)P(B)=P(AB)
$$

定理：两独立事件$A,B$的积$AB$的概率$P(AB)$等于其各自概率的积$P(A)P(B)$

在实际问题中，我们并不常用上式去判断两个事件是否独立，而是相反：从事件的实际角度去分析判断其不应有关联，因而是独立的，然后就可以用上式

多个事件独立性的定义：

概率乘法定理：

若干个独立事件$A_1,\cdots,A_n$之积的概率，等于各事件概率的乘积：

$$
P(A_1\cdots A_n)=P(A_1)\cdots P(A_n)
$$

推论：

独立事件的任一部分也独立

若一系列事件$A_1,A_2,\cdots$相互独立，则将其中任一部分改为对立事件时，所得事件列仍相互独立.

相互独立和两两独立：

由互相独立必推出两两独立；由两两独立不一定推出互相独立

例(cxr例3.4)：

$E=\{飞机被击落 \}，$ $E_0=\{驾驶员被击中 \}，E_i=\{i号发动机被击中 \}(i=1,2),$规定：$E=E_0+E_1E_2,$设$E_0,E_1,E_2$三事件独立,$P(E_0)=p_0,P(E_1)=p_1,P(E_2)=p_2$，求$P(E)$

思路：首先，要区分“独立”与“互斥”的概念.“独立”则可以用概率乘法定理，“互斥”则可以用概率加法定理.这里$E_0,E_1,E_2$三事件独立，但不互斥，因为$E_0,E_1,E_2$可以同时发生.

法一：

$$
P(E)
=P(E_0+E_1E_2)
=P(E_0)+P(E_1E_2)-P(E_0E_1E_2)
=p_0+p_1p_2-p_0p_1p_2
$$

法二(考虑逆事件)：

考虑：

$$
P(\overline{E})
=P(\overline{E_0+E_1E_2})
=P(\mathop{\overline{E_0}}\mathop{\overline{E_1E_2}})
=P(\overline{E_0})P(\overline{E_1E_2})
=(1-P(E_0))(1-P(E_1E_2))
=(1-P(E_0))(1-P(E_1)P(E_2))
=(1-p_0)(1-p_1p_2)
$$

于是：

$$
P(E)
=1-P(\overline{E})
=p_0+p_1p_2-p_0p_1p_2
$$

例(cxr例3.5,很难)：



由于$E_0,E_1,E_2$三事件独立，所以他们的逆事件也独立，于是他们的逆事件满足概率乘法定理，


全概率公式：

设$B_1,B_2,\cdots$为有限或无限个事件，他们两两互斥且在每次试验中至少发生一个.数学表达式为：

$$
B_iB_j=\varnothing,~~~(i\neq j) \\
B_1+B_2+\cdots =\Omega~~~(必然事件)
$$

有时，把具有这些性质的一组事件称为一个“**完备事件群**”.显然，任一事件$B$及其对立事件组成一个完备事件群

在上述条件下考虑任一事件$A,$有：

$$
A
=A\Omega
=A(B_1+B_2+\cdots)
=AB_1+AB_2+\cdots
$$

因为$B_1,B_2,\cdots$两两互斥，所以$AB_1,AB_2,\cdots$也两两互斥，故由加法定理，有：

$$
P(A)
=P(AB_1+AB_2+\cdots)
=P(AB_1)+P(AB_2)+\cdots
$$

由条件概率定义：$P(AB_i)=P(B_i)P(A|B_i), $有：

$$
P(A)
=P(B_1)P(A|B_1)+P(B_2)P(A|B_2)+\cdots
$$

上式就称为“**全概率公式**”

“全”部概率被分成了许多部分之和.它的理论和实用意义在于：在较复杂的情况下直接算$P(A)$不容易，但$A$总是随某个$B_i$伴出，适当构造这一组$B_i$往往可以简化运算

另一个角度理解全概率公式：把$B_i$看作导致事件$A$发生的一种可能途径
.对不同途径，$A$发生的概率，即条件概率$P(A|B_i)$各不相同，而采取哪条途径却是随机的.在直观上容易理解：在这种机制下，$A$的综合概率$P(A)$应在最小的$P(A|B_i)$和最大的$P(A|B_i)$之间,但$P(A)$也不一定是所有$P(A|B_i)$的算术平均值，因为各途径被使用的机会$P(B_i)$各不相同.$P(A)$正确的答案,应该是诸$P(A|B_i)(i=1,2,\cdots)$以$P(B_i)$为权的加权平均值

例(cxr例3.7)：

设一个家庭有$k$个小孩的概率为$p_k(k=0,1,2,\cdots).$又设各小孩的性别独立，且生男、生女的概率各为$\frac{1}{2}.$试求事件$A=\{家庭中所有小孩为统一性别\}$的概率(约定当$k=0$时,$P(A|k=0)=1$).

引进事件$B_k=\{家庭中有k各小孩 \}，$则$B_0,B_1,\cdots$构成完备事件群.由题知，$P(B_k)=p_k$.由全概率公式：$P(A)=P(B_0)P(A|B_0)+P(B_1)P(A|B_1)+\cdots$，有:

当$k\geqslant 1,$

$$
P(A|B_k)
=2\cdot\frac{1}{2^k}
=\frac{1}{2^{k-1}}
$$

于是：

$$
P(A)
=p_0+\sum_{k=1}^\infty \frac{p_k}{2^{k-1}}
$$

### 贝叶斯公式

在全概率公式假定之下，有：

$$
P(B_i|A)
=\frac{P(B_i)P(A|B_i)}{\sum\limits_{j}P(B_j)P(A|B_j)}
$$

证明：

$$
P(B_i|A)
=\frac{P(AB_i)}{P(A)}
=\frac{P(B_i)P(A|B_i)}{\sum\limits_{j}P(B_j)P(A|B_j)}
$$

对贝叶斯公式的认识：

先看$P(B_1),P(B_2),\cdots,$它们是在没有进一步的信息(不知事件$A$是否发生)的情况下，人们对诸事件$B_1,B_2,\cdots$发生可能性大小的认识；现在有了新的信息(知道$A$发生)，人们对$B_1,B_2,\cdots$发生可能性大小有了新的估价.这种情况在日常生活中也是屡见不鲜的：原以为不甚可能的一种情况，可以因某种事件的发生而变得甚为可能；或者相反.贝叶斯公式从数量上刻画了这种变化.

如果我们把事件$A$看成“结果”，把诸事件$B_1,B_2,\cdots$看成导致这个结果的可能的“原因”
，则可以形象地把**全概率公式**看作“**由原因推结果**”；而**贝叶斯公式**则恰好相反，其作用在于“**由结果推原因**”：现在有一个“结果”$A$已经发生了，在众多可能的“原因”中，到底是哪一个导致了这个结果？

例(cxr例3.9):

设某种病菌在人口中的带菌率为$0.03$,当检查时，由于各种原因，使带菌者未必检出阳性而不带菌者也可能呈阳性反应，假定：

$$
P(阳性|带菌)=0.99,P(阴性|带菌)=0.01 \\
P(阳性|不带菌)=0.05,P(阴性|不带菌)=0.95
$$

现设某人检出阳性，问“他带菌”的概率是多少？

解：

记$B_1=\{带菌\},B_2=\{不带菌\},A=\{阳性\},\bar{A}=\{阴性\}$

则已知条件可改写为：

$$
P(B_1)=0.03,P(B_2)=0.97 \\
P(A|B_1)=0.99,P(\bar{A}|B_1)=0.01 \\
P(A|B_2)=0.05,P(\bar{A}|B_2)=0.95
$$

由贝叶斯公式，得：

$$
P(B_1|A)
=\frac{P(B_1)P(A|B_1)}{P(B_1)P(A|B_1)+P(B_2)P(A|B_2)}
=\frac{0.03\times 0.99}{0.03\times 0.99+0.97\times0.05}
=0.380
$$

# 随机变量及概率分布

顾名思义，“随机变量”就是“其值随机会而定的变量”

可以说，**随机变量就是试验结果的“函数”**(从试验结果到数的映射)

随机变量：

(浙大版)设随机试验的样本空间为$S=\{e\}.X=X(e)$是定义在样本空间$S$上的实值单值函数.称$X=X(e)$为随机变量.

离散型随机变量：

称一个随机变量是离散型随机变量，若它全部可能取到的值是有限个或可列无限多个.(q:啥叫可列无限多个？)

**离散型随机变量的概率函数/概率分布**：

|可能值  |$a_1$|$a_2$|$\cdots$|$a_i$|$\cdots$|
|:------:|-------|------|------|------|------|
|**概率**|$p_1$|$p_2$|$\cdots$|$p_i$|$\cdots$|


### $(0-1)$分布：

设随机变量$X$只可能取$0,1$两个值，它的分布律是：

$$
P(X=k)
=p^k(1-p)^{1-k},k=0,1(0<p<1)
$$

则称 $X$ 服从以 $p$ 为参数的$(0-1)$分布或两点分布

**$(0-1)$分布的概率分布**：

|可能值 |0 |1 |
|:---:|:---:|:---:|
|概率|$1-p$ |$p$ |

伯努利试验:

设试验 $E$ 只有两个可能结果：$A$ 和 $\bar{A}$，则称 $E$ 为伯努利试验.设：$P(A)=p(0<p<1) ,$ 则：$P(\bar{A})=1-p $

$n$ 重伯努利试验:

将 $E$ (只有两种可能地试验)独立重复地进行 $n$ 次，则称这一串重复的独立试验为$n$**重伯努利试验**

### 二项分布：

以 $X$ 表示 $n$ 重伯努利试验中事件 $A$ 发生的次数，$p$ 为事件 $A$ 发生的概率，$X$ 是一个随机变量，我们称随机变量 $X$ 服从参数为 $n,p$ 的二项分布，记为：

$$
X\sim B(n,p)
$$

$X$的分布律为：

$$
P(X=k)
=\tbinom{n}{k}p^k(1-p)^{n-k} ，k=0,1,2,\cdots ,n
$$

### 泊松分布:

设随机变量 $X$ 所有可能的取值为 $0,1,2,\cdots$，而取各个值的概率为：
$$
P(X=k)
=\frac{\lambda^ke^{-\lambda}}{k!},k=0,1,2,\cdots
$$
其中，$\lambda>0$是常数.则称$X$服从参数为$\lambda$的泊松分布.

泊松分布的性质(想到$e^x$的泰勒展开)：

$$
\sum_{k=0}^\infty P(X=k)
=\sum_{k=0}^\infty \frac{\lambda^ke^{-\lambda}}{k!}
=e^{-\lambda}\sum_{k=0}^\infty\frac{\lambda^k}{k!}
=e^{-\lambda}e^\lambda
=1
$$

### 泊松定理(将二项分布近似为泊松分布)：

设$\lambda>0 $是一个常数，n是任意正整数，设$np_n=\lambda$,则对于任一固定的非负整数$k$,有：
$$
\lim_{n\to\infty}\tbinom{n}{k}p_n^k(1-p_n)^{n-k}
=\frac{\lambda^ke^{-\lambda}}{k!}
$$

证明：

当 $n$ 很大，$p$ 很小，为参数的二项分布的概率值可以由参数为$\lambda=np$的泊松分布的概率值近似.

### 随机变量的分布函数：

设$X$是一个随机变量，$x$是任意实数，函数：

$$
F(x)
=P(X\leqslant x),-\infty<x<+\infty
$$
称为$X$的分布函数.

对于任意实数$x_1,x_2(x_1 < x_2) $ ,有：
$$
P(x_1<X\leqslant x_2)
=p(X\leqslant x_2)-P(X\leqslant x_1)
=F(x_2)-F(x_1)
$$

连续型随机变量：

若对于随机变量$X$的分布函数$F(x)$,存在非负可积函数$f(x) $,使对于任意实数有：

$$
F(x)=\int_{-\infty}^xf(t)dt
$$

则称$X$为连续型随机变量，$f$ 称为$X$的概率密度函数，简称概率密度.

概率密度的性质：

$$
\begin{aligned}
&1.f(x)\geqslant0 \\
&2.\int_{-\infty}^{+\infty} f(x)dx=1 \\
&3.对于任意实数x_1,x_2(x_1\leqslant x_2),
P(x_1<X\leqslant x_2)=F(x_2)-F(x_1)=\int_{x_1}^{x_2}f(x)\mathrm{d}x \\
&4.若f(x)在x点连续，则有：F'(x)=f(x) \\
&5.若f(x)具有性质1,2,则可引入：
G(x)=\int_{-\infty}^x f(t)\mathrm{d}t~~. 其中，G(x)是某一随机变量X的函数，f(x)是X的概率密度.
\end{aligned}
$$

### 均匀分布：

若连续型随机变量$X$具有概率密度：

$$
f(x)=
\begin{cases}
\frac{1}{b-a}&,&a<x<b \\
0&, &其他
\end{cases}
$$

则称$X$在区间$(a,b) $上服从均匀分布，记为:
$$
X\sim U(a,b)
$$

服从均匀分布的随机变量$X$的分布函数为：

$$
F(x)=
\begin{cases}
0&,x\leqslant a \\
\frac{x-a}{b-a}&,a<x<b \\
1 &,x\geqslant b
\end{cases}
$$

### 指数分布：

(浙大版)若连续型随机变量$X$的概率密度为：

$$
f(x)=
\begin{cases}
\frac{1}{\theta}e^{-\frac{x}{\theta}}&,x>0 \\
0&,其他
\end{cases}
$$

其中$\theta>0$为常数，则称$X$服从参数为$\theta$的指数分布.

$X$的分布函数为：
$$
F(x)=
\begin{cases}
1-e^{-\frac{x}{\theta}}&,x>0 \\
0&,其他
\end{cases}
$$

(中科大版)若随机变量$X$ 有概率密度函数：

$$

f(x)
=\begin{cases}
\lambda e^{-\lambda x} &,当x>0时\\
0 &,当x\leqslant 0时
\end{cases}
$$

则称$X$服从指数分布，其中$\lambda>0$为参数

随机变量$X$的分布函数为：

$$
F(x)=
\begin{cases}
0 &,当x\leqslant 0时 \\
1-e^{-\lambda x} &,当x>0时
\end{cases}
$$

性质：

无记忆性：

若随机变量$X$服从指数分布，则对于任意$s,t>0$,有:

$$
P\{X>s+t|X>s\}
=P\{X>t\}
$$
证明：

$$
\begin{aligned}
P\{X>s+t |X>s \}
&=\frac{P\{(X>s+t)\bigcap(X>s) \}}{P\{X>s \}} \\
&=\frac{P{\{X>s+t \}}}{P\{X>s \}} \\
&=\frac{1-F(s+t)}{1-(s)} \\
&=\frac{e^{-\frac{s+t}{\theta}}}{e^{-\frac{s}{\theta}}} \\
&=e^{-\frac{t}{\theta}}
\end{aligned}
$$

而：
$$
P\{X>t \}
=e^{-\frac{t}{\theta}}
$$

故：

$$
P\{X>s+t|X>s\}
=P\{X>t\}
$$

### 正态分布：

称连续型随机变量 $X$ 服从参数为 $\mu,\sigma$ 的正态分布或高斯分布，记为$X\sim N(\mu,\sigma^2)$,若连续型随机变量的概率密度为：
$$
f(x)
=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}},-\infty<x<+\infty
$$
其中，$\mu,\sigma(\sigma>0)$为常数.

证明此概率密度满足归一化：


首先有结论：
$$
\int_{-\infty}^\infty e^{-\frac{t^2}{2}}\mathrm{d}t=\sqrt{2\pi} 
$$

性质：

曲线关于 $x=\mu$ 对称.这表明对任意 $h>0 $有：

$$
P\{\mu-h<X\leqslant\mu \}
=
P\{\mu<X\leqslant \mu+h \}
$$

当$x=\mu $时取到最大值.

在$x=\mu \pm \sigma $处曲线有拐点，曲线以$Ox$轴为渐进线.

设 $:X\sim N(\mu,\sigma^2),$ 当 $\mu=0,\sigma=1$时，称随机变量$X$服从**标准正态分布**，其概率密度和分布函数分别用$\varphi(x),\Phi(x) $表示，即有：

$$
\varphi(x)
=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}} \\
\Phi(x)
=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^x e^{-\frac{t^2}{2}}\mathrm{d}t 

$$
性质：

$$
\Phi(x)+\Phi(-x) = 1
$$

证明(利用偶函数积分性质)：

$$

\begin{aligned}

\Phi(x)+\Phi(-x)
&=\frac{1}{\sqrt{2\pi}}(\int_{-\infty}^x e^{-\frac{t^2}{2}}\mathrm{d}t +\int_{-\infty}^{-x}e^{-\frac{t^2}{2}}\mathrm{d}t ) \\
&=\frac{1}{\sqrt{2\pi}}(\int_{-\infty}^x e^{-\frac{t^2}{2}}\mathrm{d}t +\int_{x}^{+\infty}e^{-\frac{t^2}{2}}\mathrm{d}t ) \\
&=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{+\infty} e^{-\frac{t^2}{2}}\mathrm{d}t \\
&=1

\end{aligned}

$$

若随机变量$X \sim N(\mu,\sigma^2) $,则$Z=\frac{X-\mu}{\sigma} \sim N(0,1) $

证明：

$$

\begin{aligned}

P\{Z\leqslant x\}
&=P\{\frac{X-\mu}{\sigma}\leqslant x \} \\
&=P\{X\leqslant \sigma x+\mu\} \\
&=\int_{t=-\infty}^{t=\sigma x+\mu} \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(t-\mu)^2}{2\sigma^2}}\mathrm{d}t \\

\end{aligned}

$$

换元：$t=\sigma u+\mu,$当 $t=-\infty,u=-\infty;$当 $t=\sigma x+\mu,u=x,$于是：

$$

\begin{aligned}

P\{Z\leqslant x\}
&=\int_{t=-\infty}^{t=\sigma x+\mu} \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(t-\mu)^2}{2\sigma^2}}\mathrm{d}t \\
&=\int_{u=-\infty}^{u=x} \frac{1}{\sqrt{2\pi}}e^{-\frac{u^2}{2}}\mathrm{d}u

\end{aligned}

$$

于是 $Z=\frac{X-\mu}{\sigma}\sim N(0,1)$


应用：

若随机变量$X\sim N(\mu,\sigma^2)$,$X$的分布函数为$F$，则：

$$
F(x)
=P\{X\leqslant x \}
=P\{\frac{X-\mu}{\sigma}\leqslant \frac{x-\mu}{\sigma} \}
=P\{Z\leqslant \frac{x-\mu}{\sigma} \}
=\Phi(\frac{x-\mu}{\sigma})
$$

对于任意区间$(x_1,x_2]$,有：

$$
P\{x_1<X \leqslant x_2 \}
=\Phi(\frac{x_2-\mu}{\sigma})-\Phi(\frac{x_1-\mu}{\sigma})
$$

例：设：$X\sim N(1.5,2^2),$求：$P(-1\leqslant X\leqslant 2)$

思路：

解：

$$
\begin{aligned}
P(-1\leqslant X\leqslant 2)
&=P(\frac{-1-1.5}{2}\leqslant \frac{X-1.5}{2}\leqslant \frac{2-1.5}{2}) \\
&=P(-1.25\leqslant \frac{X-1.5}{2}\leqslant 0.25) \\
&=\Phi(0.25)-\Phi(-1.25)
\end{aligned}
$$




### 随机变量函数的分布：

离散型随机变量：

连续型随机变量：

定理：

SOP:

开始，先由$X,Y$关系确定$Y$的取值范围.如：$Y=X^2,有Y\geqslant 0$则当$y\leqslant 0,F_Y(y)=P\{Y\leqslant y \}=0$或者直接$f_Y(y)=0$?；如$Y=\sqrt{X},有Y\geqslant 0 $


接着求$Y$的分布函数$F_Y(y)=P\{Y\leqslant y\}$，由$X,Y$关系把$Y$用$X$表达，$P\{Y\leqslant y\}=P\{h(X)\leqslant y \}=P\{X\leqslant h^{-1}(y)\}=F_X(h^{-1}(y))$于是，$F_Y(y)=F_X(h^{-1}(y))$，最后两边对$y$求导：$f_Y(y)=f_X(h^{-1}(y))\cdot\frac{\mathrm{d}}{\mathrm{d}y}h^{-1}(y) $


例（已知一个随机变量$X$的概率密度、另一随机变量$Y$与$X$的关系式，求$Y$的概率密度）：

设随机变量$X$具有概率密度：

$$
f_X(x)=
\begin{cases}
\frac{x}{8}&,0<x<4 \\
0&,其他
\end{cases}
$$

求随机变量$Y=2X+8$的概率密度

记$X,Y$的分布函数分别为：$F_X(x),F_Y(y)$

$$
F_Y(y)
=P\{Y\leqslant y\}
=P\{2X+8\leqslant y \}
=P\{X\leqslant \frac{y-8}{2} \}
=F_X(\frac{y-8}{2})
$$

两边同时对$y$求导：

$$
f_Y(y)
=f_X(\frac{y-8}{2})(\frac{y-8}{2})'
=
\begin{cases}
\frac{1}{2}\frac{\frac{y-8}{2}}{8} &, 0<\frac{y-8}{2} <4 \\
0 &,其他
\end{cases}
=\begin{cases}
\frac{y-8}{32} &, 8<y<16 \\
0 &, 其他
\end{cases}
$$

例：

设随机变量$X$具有概率密度$f_X(x),-\infty<x<\infty $,求 $Y=X^2$ 的概率密度

当$Y\leqslant 0,F_Y(y)=0,$或者直接$f_Y(y)=0$更直接？

当$Y\geqslant 0,$

$$
F_Y(y)
=P\{Y\leqslant y\}
=P\{X^2\leqslant y\}
=P\{-\sqrt{y}\leqslant X\leqslant\sqrt{y}\}
=F_X(\sqrt{y})-F_X(-\sqrt{y})
$$

两边对$y$求导：

当$y>0,$

$$
f_Y(y)
=\frac{1}{2\sqrt{y}}\bigg(f_X(\sqrt{y})+f_X(\sqrt{-y})\bigg)
$$

综上，

$$

f_Y(y)=
\begin{cases}
\frac{1}{2\sqrt{y}}\bigg(f_X(\sqrt{y})+f_X(\sqrt{-y})\bigg) &,y>0 \\
0 &,y\leqslant 0 
\end{cases}
$$

例：

设随机变量$X\sim N(\mu,\sigma^2)$,证明$Z=\frac{x-\mu}{\sigma}\sim N(0,1) $

(浙大版2.35):

(1):

由于 $Y=e^X>0,$ 于是 $f_Y(y)=0(y\leqslant 0)$

当 $y>0,$

$$
F_Y(y)
=P(Y\leqslant y)
=P(e^X\leqslant y)
=P(X\leqslant \ln y)
=F_X(\ln y)
$$

两边同时对 $y$ 求导得：

$$
f_Y(y)
=\frac{1}{y}f_X(\ln y)
=\frac{1}{y}\frac{1}{\sqrt{2\pi}}e^{-\frac{(\ln y)^2}{2}}
$$

综上，

(2):

由于 $Y=2X^2+1\geqslant 1,$于是当 $y<1$ 时，$f_Y(y)=0$

当 $Y\geqslant 1$ 时，

$$
F_Y(y)
=P(Y\leqslant y)
=P(2X^2+1\leqslant y)
=P(-\sqrt{\frac{y-1}{2}}\leqslant X\leqslant \sqrt{\frac{y-1}{2}})
=F_X(\sqrt{\frac{y-1}{2}})-F_X(-\sqrt{\frac{y-1}{2}})
$$

两边同时对 $y$ 求导得：

$$

$$

# 多维随机变量及其分布

设$E$是一个随机试验，它的样本空间是$S=\{e\}$,设$X=X(e)$和$Y=Y(e)$是定义在$S$上的随机变量，由它们构成的一个向量$(X,Y) $叫做**二维随机向量**或**二维随机变量**

二维随机变量的**分布函数**/两个随机变量的**联合分布函数** ：

设$(X,Y) $是二维随机变量，对于任意实数$x,y$,二元函数：

$$
F(x,y)=P\{(X\leqslant x)\bigcap (Y\leqslant y) \}\stackrel{记成}{=}P\{X\leqslant x,Y\leqslant y \}
$$

称为二维随机变量$(X,Y) $的**分布函数**，或称为随机变量$X$和$Y$的联合分布函数.

几何意义：

若把二维随机变量$(X,Y) $看作平面上随机点的坐标，那么分布函数$F(x,y) $在$(x,y) $处的函数值就是随机点$(X,Y) $落在以点$(x,y)$为顶点而位于该点左下方的无穷矩形域内的概率.

借助几何图像，易知：

随机点$(X,Y)$落在矩形域$\{(x,y)|x_1<x\leqslant x_2,y_1<y\leqslant y_2 \} $的概率为：

$$
\begin{aligned}
&P\{x_1<X\leqslant x_2,y_1<Y\leqslant y_2 \ \} \\
=&F(x_2,y_2)-F(x_1,y_2)-F(x_2,y_1)+F(x_1,y_1)
\end{aligned}
$$

分布函数$F(x,y) $的基本性质：

$F(x,y) $是变量$x,y$的不减函数

$0\leqslant F(x,y)\leqslant 1 $

对于任意固定的$y,F(-\infty,y)=0 $

对于任意固定的$x,F(x,-\infty)=0 $

$F(-\infty,-\infty)=0,F(+\infty,+\infty)=1$

$F(x,y) $关于 $x$ 右连续，关于 $y$ 也右连续

对于任意$(x_1,y_1),(x_2,y_2),x_1<x_2,y_1<y_2 $,下述不等式成立：

$$
F(x_2,y_2)+F(x_1,y_1)-F(x_1,y_2)-F(x_2,y_1)\geqslant 0
$$

若二维随机变量$(X,Y)$全部可能取到的值是有限对或可列无限对，则称$(X,Y) $是二维离散型随机变量.

设二维随机变量$(X,Y)$所有可能取值为：$(x_i,y_i) ,i,j=1,2,\cdots$
记$P=\{X=x_i,Y=y_i \}=p_{ij},i,j=1,2,\cdots,$则由概率定义：

$$
p_{ij}\geqslant 0,\sum_{i=0}^\infty\sum_{j=0}^\infty p_{ij} =1
$$

我们称$P=\{X=x_i,Y=y_i \}=p_{ij},i,j=1,2,\cdots$为二维离散型随机变量$(X,Y) $的分布律，或称为随机变量$X$和$Y$的联合分布律.

求离散型二维随机变量$(X,Y)$分布律SOP：

1.写出$X,Y$各自可能取值，并制成表

2.计算特定概率

例：

离散型随机变量$X$和$Y$的联合分布函数：

$$
F(x,y)
=\sum_{x_i\leqslant x}\sum_{y_j\leqslant y}p_{ij}
$$

其中，和式是对一切满足$x_i\leqslant x,y_j\leqslant y $的$i,j$来求和的.

对于二维随机变量$(X,Y) $分布函数$F(x,y) ,$若存在非负可积函数$f(x,y)$使对于任意$x,y$有：

$$
F(x,y)
=\int_{-\infty}^y\int_{-\infty}^x f(u,v)\mathrm{d}u\mathrm{d}v
$$

则称$(X,Y) $是二维连续型随机变量,函数$f(x,y) $称为二维连续型随机变量$(X,Y)$的概率密度，或称为随机变量$X$和$Y$的联合概率密度.

概率密度$f(x,y) $的性质：

$$f(x,y)\geqslant 0$$

$$
\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty} f(u,v)\mathrm{d}u\mathrm{d}v
=1
$$

(important!)设$G$是$xOy$平面上的区域，点$(X,Y) $落在$G$内的概率为：

$$
P\{(X,Y)\in G \}
=\iint\limits_{G} f(x,y)\mathrm{d}x\mathrm{d}y
$$

若$f(x,y) $在点$(x,y) $处连续，则有：

$$
\frac{\partial^2F(x,y) }{\partial x\partial y}=f(x,y)
$$

由性质4：

在$f(x,y) $的连续点处有：
$$
\lim_{\Delta x\to 0^+\Delta y\to 0^+} \frac{P\{x<X\leqslant x+\Delta x,y<Y\leqslant y+\Delta y \}}{\Delta x\Delta y}
=\frac{\partial^2F(x,y) }{\partial x\partial y}=f(x,y)
$$

这表示，若$f(x,y)$在点$(x,y) $处连续，则当$\Delta x,\Delta y $很小时，有：

$$
P\{x<X\leqslant x+\Delta x,y<Y\leqslant y+\Delta y \}
\approx
f(x,y)\Delta x\Delta y
$$

也就是点$(X,Y) $落在小矩形$(x,x+\Delta x]\times(y,y+\Delta y] $内的概率接近$f(x,y)\Delta x\Delta y $

在几何上，$z=f(x,y) $表示空间的一个曲面.由性质2知，介于它和$xOy$平面的空间区域的体积为$1$.由性质3，$P\{(X,Y)\in G \} $的值等于以$G$为底，以曲面$z=f(x,y) $为顶面的柱体体积.


一般，设$E$是一个随机试验，它的样本空间是$S=\{e\}$,设$X_1=X_1(e),X_2=X_2(e),\cdots,X_n=X_n(e) $是定义在$S$上的随机变量，由它们构成一个$n$维向量$(X_1,X_2,\cdots,X_n)$称为$n$维随机向量或$n$维随机变量.

对于任意$n$个实数$x_1,x_2,\cdots,x_n ,n$元函数：

$$
F(x_1,x_2,\cdots,x_n)=P\{X_1\leqslant x_1,X_2\leqslant x_2,\cdots,X_n\leqslant x_n \}
$$

称为$n$维随机变量$(X_1,X_2,\cdots,X_n)$的分布函数或随机变量$X_1,X_2,\cdots,X_n$的联合分布函数.

## 边缘分布

边缘分布函数：

边缘概率密度：

二维正态分布：

$$
(X,Y)\sim N(\mu_1,\mu_2,\sigma_1,\sigma_2.\rho)
$$

概率密度为：

$$
f(x,y)
=\frac{1}{2\pi \sigma_1\sigma_2\sqrt{1-\rho^2}}\exp\{\frac{-1}{2(1-\rho^2)}[\frac{(x-\mu_1)^2}{\sigma_1^2}-2\rho\frac{(x-\mu_1)(y-\mu_2)}{\sigma_1\sigma_2}+\frac{(y-\mu_2)^2}{\sigma_2^2}] \}
$$

二维正态分布的两个边缘分布都是一维正态分布，并且不依赖于参数 $\rho$

参数 $\rho $ 就是 $X$ 和 $Y$ 的相关系数

有限个相互独立的正态随机变量的线性组合仍然服从正态分布

设 $X,Y$ 相互独立且 $X\sim N(\mu_1,\sigma_1^2),Y\sim N(\mu_2,\sigma_2^2),$则：$Z=X+Y$ 仍服从正态分布。且 $Z=X+Y\sim N(\mu_1+\mu_2,\sigma_1^2+\sigma_2^2)$

证明：



### 条件分布：

若$P\{Y=y_j\}>0$，则在$Y=y_j$条件下随机变量$X$的条件分布律为：

$$
P\{X=x_i|Y=y_j\}
=\frac{P\{X=x_i,Y=y_j \}}{P\{Y=y_j\}}
=\frac{p_{ij}}{p_{\cdot j}},
i=1,2,\cdots.
$$

若$P\{X=x_i \}>0$，则在$X=x_i$条件下随机变量$Y$的条件分布律为：

$$
P\{Y=y_j|X=x_i\}=\frac{P\{X=x_i,Y=y_j \}}{P\{X=x_i\}}=\frac{p_{ij}}{p_{i\cdot}} ,j=1,2,\cdots
$$



条件概率的性质：

1.

2.

$$
\sum_{i=1}^{\infty}P\{X=x_i|Y=y_j \}=1
$$

条件分布律：

条件概率密度：

若对于固定的$Y,f_Y(y)>0,$则称$\frac{f(x,y)}{f_Y(y)} $为在 $Y=y$ 条件下 $X$ 的条件概率密度，记为：

$$
f_{X|Y}(x|y)=\frac{f(x,y)}{f_Y(y)}
$$

称$\int_{-\infty}^x f_{X|Y}(x|y)\mathrm{d}x=\int_{-\infty}^x \frac{f(x,y)}{f_Y(y)}\mathrm{d}x $为在$Y=y$条件下$X$的**条件分布函数**，记为:$P\{X\leqslant x|Y=y\} $或$F_{X|Y}(x,y) $,即：

$$
F_{X|Y}(x,y)=P\{X\leqslant x|Y=y \}=\int_{-\infty}^x \frac{f(x,y)}{f_Y(y)}\mathrm{d}x
$$

类似地，可以定义：

条件分布函数：

均匀分布：



### 相互独立的随机变量



### 两个随机变量的函数分布

#### $Z=X+Y$ 的分布

设$(X,Y)$是二维连续型随机变量，它具有概率密度$f(x,y) $，则$Z=X+Y$仍为随机变量，其概率密度为：

$$
f_{X+Y}(z)=\int_{-\infty}^{+\infty} f(x,z-x)\mathrm{d}x 
$$

$$
或
$$

$$
f_{X+Y}(z)=\int_{-\infty}^{+\infty} f(z-y,y)\mathrm{d}y
$$

证明：

$$
F_{X+Y}(z)
=P\{X+Y\leqslant z\}
=\underset{x+y\leqslant z}{\iint} f(x,y)\mathrm{d}x\mathrm{d}y
=
$$

卷积公式：

设随机变量$X,Y$相互独立，二维随机变量$(X,Y)$的边缘概率密度分别为 $f_X(x),f_Y(y)$,则：

$$
f_{X+Y}(z)=\int_{-\infty}^{+\infty} f_X(x)f_Y(z-x)\mathrm{d}x
$$

$$
或
$$

$$
f_{X+Y}(z)=\int_{-\infty}^{+\infty} f_X(z-y)f_Y(y)\mathrm{d}y
$$

卷积公式记为：

$$
f_X*f_Y
$$

#### $Z=\frac{Y}{X}$的分布、$Z=XY$的分布

设$(X,Y)$是二维连续型随机变量，它具有概率密度$f(x,y),$则$Z=\frac{Y}{X},Z=XY $仍为连续型随机变量，其概率密度分别为：

$$
f_{\frac{Y}{X}}(z)
=\int_{-\infty}^{+\infty} |x|f(x,xz)\mathrm{d}x
$$

$$
f_{XY}(z)
=\int_{-\infty}^{+\infty} \frac{1}{|x|}f(x,\frac{z}{x})\mathrm{d}x
$$

证明：

设$(X,Y)$是二维连续型随机变量，若$X,Y$相互独立，设$(X,Y)$关于$X,Y$的边缘概率密度分别为：$f_X(x),f_Y(y) ,$则：

$$
f_{\frac{Y}{X}}(z)
=\int_{-\infty}^\infty |x|f_X(x)f_Y(xz)\mathrm{d}x
$$

$$
f_{XY}(z)
=\int_{-\infty}^\infty \frac{1}{|x|}f_X(x)f_Y(\frac{z}{x})\mathrm{d}x
$$

例：

设随机变量$(X,Y)$的概率密度为：

$$
f(x,y)=
\begin{cases}
x+y &,0<x<1,0<y<1 \\
0 &,其他
\end{cases}
$$

求$Z=XY$的概率密度

$$
f_{XY}(z)
=\int_{-\infty}^\infty \frac{1}{|x|}f(x,\frac{z}{x})\mathrm{d}x
=\begin{cases}
2-2z  &,0<z<1\\
0 &,其他
\end{cases}
$$

#### $M=\max\{ X,Y\}$及$N=\min\{X,Y \} $的分布

设$X,Y$是两个相互独立的随机变量，它们的分布函数分别为$F_X(x),F_Y(y) $,则：

$$
F_{\max}(z)
=F_X(z)F_Z(z)
$$

$$
F_{\min}(z)
=1-[1-F_X(z)][1-F_Y(z)]
$$

# 随机变量的数字特征

### 数学期望

设离散型随机变量$X$的分布律为：

$$

$$

$$
E(X)
=a_1p_1+a_2p_2+\cdots+a_mp_m
$$

设$X$有概率密度$f(x),$若：

则$X$的数学期望，记为$E(X)$,可定义为：

$$
E(X)
=\int_{-\infty}^{+\infty} xf(x)\mathrm{d}x
$$

数学期望的性质：

若干个随机变量之和的期望等于各随机变量的期望之和. 数学表达式：

$$
E(X_1+X_2+\cdots+X_n)=E(X_1)+E(X_2)+\cdots+E(X_n)
$$

若干个**独立**随机变量之积的期望等于各变量的期望之积. 数学表达式：

若$X_1,X_2,\cdots,X_n$相互独立，则：

$$
E(X_1X_2\cdots X_n)
=E(X_1)E(X_2)\cdots E(X_n)
$$


### 随机变量函数的期望


设随机变量$X$为离散型，有分布$P(X=a_i)=p_i(i=1,2,\cdots);$或者$X$为连续型，有概率密度$f(x),$则：

$$
E(g(X))
=\sum_{i} g(a_i)p_i
$$

或

$$
E(g(X))
=\int_{-\infty}^{+\infty} g(x) f(x)\mathrm{d}x
$$

推论：

若$a,b$为常数，则：

$$
E(aX+b)=aE(X)+b
$$

### 方差

设$X$为随机变量，其分布为$F,$，其方差，记为$D(X),$(或记为$D(X)$)的定义为：

$$
D(X)
=E\bigg(\big(X-E(X)\big)^2 \bigg)
$$

标准差(又称**均方差**)：$\sqrt{D(X)}$

方差计算：

$$
D(X)
=E(X^2)-E^2(X)
$$

方差的性质：

常数的方差为零

若$a,b$为常数，则：

$$
D(aX+b)
=a^2D(X)
$$

设 $X,Y$ 是两个随机变量，则有：

$$
D(X+Y)
=D(X)+D(Y)+2E\bigg([X-E(X)][Y-E(Y)] \bigg)
$$

特别地，若 $X,Y$ 相互独立，则有：

$$
D(X+Y)=D(X)+D(Y)
$$

证明：

$$

\begin{aligned}

D(X+Y)
&=E\bigg(\big[X+Y-E(X+Y)\big]^2 \bigg) \\
&=E\bigg(\big[X+Y-E(X)-E(Y)\big]^2 \bigg) \\
&=E\bigg(\big[X-E(X)+Y-E(Y) \big]^2 \bigg) \\
&=E\bigg(\big[X-E(X)\big]^2 \bigg)+E\bigg(\big[Y-E(Y) \big]^2 \bigg)+2E\bigg([X-E(X)][Y-E(Y)] \bigg) \\
&=D(X)+D(Y)+2E\bigg([X-E(X)][Y-E(Y)] \bigg)

\end{aligned}

$$

注意到：

$$

\begin{aligned}

E\bigg([X-E(X)][Y-E(Y)] \bigg)
&=E\bigg(XY-XE(Y)-YE(X)+E(X)E(Y) \bigg) \\
&=E(XY)-E(X)E(Y)-E(Y)E(X)+E(X)E(Y) \\
&=E(XY)-E(X)E(Y)

\end{aligned}

$$

若 $X,Y$ 相互独立，则：$E(XY)=E(X)E(Y),$结合上式，代入得：

$$

\begin{aligned}

D(X+Y)
&=D(X)+D(Y)+2E\bigg([X-E(X)][Y-E(Y)] \bigg) \\
&=D(X)+D(Y)+2E(XY)-2E(X)E(Y) \\
&=D(X)+D(Y)+2E(X)E(Y)-2E(X)E(Y) \\
&=D(X)+D(Y)

\end{aligned}

$$




定理：

**独立**随机变量之和的方差等于各变量的方差之和. 数学表达式：

若$X_1,X_2,\cdots,X_n$相互独立，则：

$$
D(X_1+X_2+\cdots+X_n)
=D(X_1)+D(X_2)+\cdots+D(X_n)
$$

#### 典型概率分布的期望与方差

|分布名称|分布律/概率密度|数学期望|方差|
|---|---|:---:|:---:|
|(0-1)分布 | |$p$ |$p(1-p)$ |
|二项分布| |$np$ |$np(1-p)$ |
|泊松分布 | |$\lambda$ |$\lambda$ |
|均匀分布 | |$\frac{a+b}{2}$ |$\frac{(b-a)^2}{12}$ |
|指数分布 | |$\frac{1}{\lambda} $ |$\frac{1}{\lambda^2} $ |
|正态分布 | |$\mu$ |$\sigma^2$ |

证明：

**二项分布**：
法一：

记：$X=X_1+\cdots+X_n，$其中，$\begin{cases}0,若第i次试验事件A不发生 \\ 1,若第i次试验事件A发生 \end{cases} ，$则由二项分布$X_1,\cdots,X_n$独立，且$E(X_i)=0\cdot(1-p)+1\cdot p=p,$有：

$$
E(X)
=E(X_1)+\cdots+E(X_n)
=\underbrace{p+\cdots+p}_{n个p}
=np
$$

法二：

注意到：

$$
i\cdot\tbinom{n}{i}
=i\cdot\frac{n!}{i!(n-i)!}
=\frac{n!}{(i-1)!\bigg((n-1)-(i-1)\bigg)!}
=n\cdot\frac{(n-1)!}{(i-1)!\bigg((n-1)-(i-1)\bigg)!}
=n\cdot\tbinom{n-1}{i-1}
$$

于是：

$$
E(X)
=\sum_{i=0}^{n} i\cdot\tbinom{n}{i}p^i(1-p)^{n-i}
=\sum_{i=1}^{n} i\cdot\tbinom{n}{i}p^i(1-p)^{n-i}
=\sum_{i=1}^{n} n\cdot\tbinom{n-1}{i-1}p^i(1-p)^{n-i}
=np\sum_{i=1}^{n} \tbinom{n-1}{i-1} p^{i-1}(1-p)^{(n-1)-(i-1)}
=np(p+(1-p))^{n-1}
=np
$$

法一：

记：$X=X_1+\cdots+X_n，$其中，$\begin{cases}0,若第i次试验事件A不发生 \\ 1,若第i次试验事件A发生 \end{cases} ，$独立，且$D(X_i)=E(X_i^2)-E^2(X_i)=p-p^2,$得：

$$
D(X)
=D(X_1+\cdots+X_n)
=D(X_1)+\cdots+D(X_n)
=\underbrace{p-p^2+\cdots+p-p^2}_{n个p-p^2}
=n(p-p^2)
=np(1-p)
$$

法二：

$$
D(X)
=E(X^2)-E^2(X)
=\bigg(\sum_{i=0}^{n} i^2 \cdot \tbinom{n}{i} p^i(1-p)^{n-i} \bigg) -n^2p^2
=-n^2p^2+\sum_{i=1}^{n} i^2\cdot\tbinom{n}{i} p^i(1-p)^{n-i}
=
$$

**泊松分布**:

$$
E(X)
=\sum_{i=0}^{\infty} i\frac{\lambda^i e^{-\lambda}}{i!}
=\sum_{i=1}^\infty i\frac{\lambda^ie^{-\lambda}}{i!}
=e^{-\lambda}\sum_{i=1}^\infty \frac{\lambda^{i-1}\cdot\lambda}{(i-1)!}
=\lambda e^{-\lambda}\sum_{i=1}^\infty\frac{\lambda^{i-1}}{(i-1)!}
=\lambda e^{-\lambda}\cdot e^{\lambda}
=\lambda
$$

$$
D(X)
=E(X^2)-E^2(X)
=\sum_{i=0}^\infty i^2\frac{\lambda^i e^{-\lambda}}{i!} -\lambda^2
=\sum_{i=1}^\infty i^2\frac{\lambda^i e^{-\lambda}}{i!} -\lambda^2
=-\lambda^2+e^{-\lambda}\sum_{i=1}^\infty i\frac{\lambda^i}{(i-1)!} 
$$

**均匀分布**：

$$
E(X)
=\int_{-\infty}^{+\infty} x f(x)\mathrm{d}x
=\int_{a}^{b} x\frac{1}{b-a}\mathrm{d}x
=\frac{a+b}{2}
$$

$$
D(X)
=E(X^2)-E^2(X)
=\int_{-\infty}^{+\infty} x^2 f(x)\mathrm{d}x -\bigg(\frac{a+b}{2}\bigg)^2
=-\bigg(\frac{a+b}{2}\bigg)^2+ \int_a^b x^2\frac{1}{b-a}\mathrm{d}x
=\frac{(b-a)^2}{12}
$$

指数分布:

$$
E(X)
=\int_{-\infty}^{+\infty} xf(x)\mathrm{d}x
=\int_{0}^{+\infty} x\cdot\lambda e^{-\lambda x}\mathrm{d}x
=\frac{\lambda}{-\lambda}\int_{0}^{+\infty} x\mathrm{d}(e^{-\lambda x}) 
=-(xe^{-\lambda x}\big|_{x=0}^{x=+\infty})+\int_{0}^{+\infty} e^{-\lambda x}\mathrm{d}x
=\frac{1}{\lambda}
$$

$$
D(X)
=E(X^2)-E^2(x)
=\int_{-\infty}^{+\infty} x^2 f(x)\mathrm{d}x-\frac{1}{\lambda^2}
=-\frac{1}{\lambda^2}+ \int_{0}^{+\infty} x^2\lambda e^{-\lambda x}\mathrm{d}x
=-\frac{1}{\lambda^2}+\frac{2}{\lambda^2}
=\frac{1}{\lambda^2}
$$

**瑞利分布**（Rayleigh distribution）：

当一个随机二维向量的两个分量相互独立，且都服从均值为 $0 $、并有相同方差的正态分布时，这个向量的模服从瑞利分布。

瑞利分布的概率密度为：

$$
f(z)
=\left\{
\begin{aligned}
&\frac{z}{\sigma^2}\mathrm{e}^{-z^2/(2\sigma^2)}&&,~~z>0 \\
&0&&,~~z\leqslant 0
\end{aligned}
\right.
$$

可以验证，瑞利分布概率密度满足归一性：

$$
\begin{aligned}
\int_{-\infty}^{+\infty} f(z)\mathrm{d}z
&=\int_{0}^{+\infty} \frac{z}{\sigma^2}\mathrm{e}^{-z^2/(2\sigma^2)} \mathrm{d}z \\
&=-\int_{0}^{+\infty}\mathrm{e}^{-z^2/(2\sigma^2)} \mathrm{d}(\frac{-z^2}{2\sigma^2}) \\
&=-\mathrm{e}^{-z^2/(2\sigma^2)}\bigg|_{z=0}^{z=+\infty} \\
&=1
\end{aligned}
$$

服从瑞利分布的随机变量的平均值：

$$
\begin{aligned}
\bar{Z}
&\equiv \int_{z=-\infty}^{z=+\infty} z f(z)\mathrm{d}z \\
&=\int_{0}^{+\infty} \frac{z^2}{\sigma^2} \mathrm{e}^{-z^2/(2\sigma^2)} \mathrm{d}z \\
&=\frac{-1}{\sigma^2} \int_{0}^{+\infty} \frac{\partial}{\partial(1/2\sigma^2)} \exp(-\frac{z^2}{2\sigma^2})\mathrm{d}z \\
&=\frac{-1}{\sigma^2} \frac{\mathrm{d}}{\mathrm{d}(1/2\sigma^2)}\int_{0}^{+\infty} \exp(-\frac{z^2}{2\sigma^2})\mathrm{d}z \\
&=\frac{-1}{\sigma^2} \frac{\mathrm{d}}{\mathrm{d}(1/2\sigma^2)} (\sqrt{\frac{\pi}{2}}\sigma) \\
&=\frac{-\sqrt{\pi}}{2\sigma^2}\frac{\mathrm{d}}{\mathrm{d}(1/2\sigma^2)} (\frac{1}{2\sigma^2})^{-1/2} \\
&=\frac{-\sqrt{\pi}}{2\sigma^2} \cdot(-\frac{1}{2})(\frac{1}{2\sigma^2})^{-3/2} \\
&=\sqrt{\frac{\pi}{2}} \sigma
\end{aligned}
$$

方差：

$$
\begin{aligned}
D(z)
&=(2-\frac{\pi}{2})\sigma^2
\end{aligned}
$$

瑞利分布概率密度的推导：

设 $X\sim N(0,\sigma^2),Y\sim N(0,\sigma^2) $，$X,Y $ 相互独立，$Z\equiv X+\mathrm{i} Y $，$|Z|=\sqrt{X^2+Y^2} $

$$
\begin{aligned}
F(z)
&\equiv P(|Z|<z) \\
&=P(\sqrt{X^2+Y^2}<z) \\
&=\iint\limits_{\sqrt{x^2+y^2}<z} f(x,y)\mathrm{d}x\mathrm{d}y \\
&=\iint\limits_{r<z} \frac{1}{2\pi\sigma^2}\exp(-\frac{r^2}{2\sigma^2}) r\mathrm{d}r\mathrm{d}\theta \\
&=\frac{1}{\sigma^2}\int_{r=0}^{r=z}r\cdot\exp(-\frac{r^2}{2\sigma^2})\mathrm{d}r \\
&=-\int_{r=0}^{r=z}\exp(-\frac{r^2}{2\sigma^2})\mathrm{d}(-\frac{r^2}{2\sigma^2}) \\
&=-\mathrm{e}^{-r^2/(2\sigma^2)}\bigg|_{r=0}^{r=z} \\
&=1-\mathrm{e}^{-z^2/(2\sigma^2)},~~z>0
\end{aligned}
$$

于是概率密度为：

$$
\begin{aligned}
f(z)
&=\frac{\mathrm{d}F(z)}{\mathrm{d}z} \\
&=\frac{z}{\sigma^2} \mathrm{e}^{-z^2/(2\sigma^2)},~~z>0
\end{aligned}
$$



### 矩

设$X$为随机变量，$c$为常数，$k$为正整数. $X$关于$c$点的$k$阶矩，定义为：

$$
E\bigg((X-c)^k\bigg)
$$

两种特殊情况：

## 协方差与相关系数

协方差定义：

$$
\mathrm{Cov}(X,Y)
=E\bigg(\big(X-E(X)\big)\big(Y-E(Y)\big)\bigg)
$$

**协方差的性质**：

$$
\mathrm{Cov}(c_1X+c_2,c_3Y+c_4)
=c_1c_3\mathrm{Cov}(X,Y)
$$

$$
\mathrm{Cov}(X,Y)=E(XY)-E(X)E(Y)
$$

$$
D(X+Y)
=D(X)+D(Y)+2\mathrm{Cov}(X,Y)
$$

$$
\mathrm{Cov}(X_1+X_2,Y)
=\mathrm{Cov}(X_1,Y)+\mathrm{Cov}(X_2,Y)
$$

定理：

若$X,Y$ 相互独立，则：$\mathrm{Cov}(X,Y)=0$

$[\mathrm{Cov}(X,Y)]^2\leqslant \sigma_1^2\sigma_2^2 .$等号成立当且仅当$X,Y$之间存在严格线性关系(即存在常数$a,b$使得$Y=aX+b$)

相关系数($\mathrm{corr}(X,Y)$或$\rho_{XY}$)：

$$
\rho_{XY}
=\frac{\mathrm{Cov}(X,Y)}{\sqrt{D(X)}\sqrt{D(Y)}}
$$

定理：

若$X,Y$独立，则：$\rho_{XY}=0$

$|\rho_{XY}|\leqslant 1, $等号取到当且仅当$X,Y$有严格线性关系

称$X,Y$不相关，若$Corr(X,Y)=0$或$Cov(X,Y)=0$

定理一说，$X,Y$独立能推出他们不相关，但反过来一般不成立

(浙大版4.9)

(1):

思路：由公式： $E(g(X,Y))=\iint\limits_{D} g(x,y)f(x,y)\mathrm{d}x\mathrm{d}y $计算

$$
E(X)
=\iint\limits_{D} xf(x,y)\mathrm{d}x\mathrm{d}y
=\frac{4}{5}
$$

$$
E(Y)
=\iint\limits_{D} yf(x,y)\mathrm{d}x\mathrm{d}y
=\frac{3}{5}
$$

$$
E(XY)
=\iint\limits_{D} xyf(x,y)\mathrm{d}x\mathrm{d}y
=\frac{1}{2}
$$

$$
E(X^2+Y^2)
=\iint\limits_{D} (x^2+y^2)f(x,y)\mathrm{d}x\mathrm{d}y
=\frac{16}{15}
$$

(浙大版28):

思路：要证明两个随机变量不相关，只要证明它们的协方差为零；要证明两个随机变量不是相互独立的，只要证明它们的联合概率密度不等于边缘概率密度之积

解：

记：$D=\{(x+y)|x^2+y^2\leqslant 1 \} $

$$
cov(X,Y)
=E(XY)-E(X)E(Y)
$$

$$
E(XY)
=\iint\limits_{\mathbb{R}^2} xyf(x,y)\mathrm{d}x\mathrm{d}y
=\iint\limits_{D} xy\frac{1}{\pi}\mathrm{d}x\mathrm{d}y
=0 
$$

$$
E(X)
=\iint\limits_{\mathbb{R}^2} xf(x,y)\mathrm{d}x\mathrm{d}y
=0
$$

$$
E(Y)
=E(X)
=0
$$

于是:

$$
cov(X,Y)=0
$$

因为 $X,Y$ 的协方差为零，故 $X,Y$ 不相关

$$
f_X(x)
=\int_{-\infty}^{+\infty} f_{XY}(x,y)\mathrm{d}y
=\frac{2}{\pi}\sqrt{1-x^2}
$$

$$
f_Y(y)
=\frac{2}{\pi}\sqrt{1-y^2}
$$

$$
f_{XY}(x,y)
\ne f_X(x)f_Y(y)
$$

于是$X,Y$ 不相互独立

(浙大版4.32):

$$
E(X)
=\iint\limits_{\mathbb{R}^2} xf(x,y)\mathrm{d}x\mathrm{d}y
=\int_{x=0}^{x=2}\mathrm{d}x\int_{y=0}^{y=2}x\cdot\frac{1}{8}(x+y)\mathrm{d}y
=\frac{7}{6}
$$

$$
E(Y)
=\iint\limits_{\mathbb{R}^2} yf(x,y)\mathrm{d}y
=\frac{7}{6}
$$

$$
E(XY)
=\iint\limits_{\mathbb{R}^2} xyf(x,y)\mathrm{d}x\mathrm{d}y
=\frac{4}{3}
$$

$$
Cov(X,Y)
=E(XY)-E(X)E(Y)
=\frac{4}{3}-\frac{7}{6}\cdot\frac{7}{6}
=-\frac{1}{36}
$$

$$
E(X^2)
=\iint\limits_{\mathbb{R}^2} x^2f(x,y)\mathrm{d}x\mathrm{d}y
=\frac{5}{3}
$$

$$
E(Y)=E(X)=\frac{5}{3}
$$

$$
D(X)
=E(X^2)-E^2(X)
=\frac{11}{36}
$$

$$
D(Y)=D(X)=\frac{11}{36}
$$

$$
\rho_{XY}
=\frac{Cov(X,Y)}{\sqrt{D(X)}\sqrt{D(Y)}}
=\frac{-\frac{1}{36}}{\sqrt{\frac{11}{36}}\sqrt{\frac{11}{36}}}
=-\frac{1}{11}
$$

$$
D(X+Y)
=D(X)+D(Y)+2Cov(X,Y)
=\frac{11}{36}+\frac{11}{36}+2(-\frac{1}{36})
=\frac{5}{9}
$$




# 大数定理和中心极限定理

**独立同分布**：称随机变量$X_1,\cdots,X_n,\cdots$独立同分布，若$X_1,\cdots,X_n,\cdots$互相独立，且服从同一分布

$\bar{X}_n$的定义：

定义$X_i$为：

$$
X_i=

\begin{cases}

1&,若在第i次试验时事件A发生 \\
0&,若在第i次试验时事件A不发生

\end{cases}

$$

在上面对$X_i$的定义之下，定义频率$\bar{X}_n$:

$$
\bar{X}_n=\frac{X_1+\cdots+X_n}{n}
$$

### 马尔可夫不等式

若$Y$为只取非负值的随机变量，则对任意常数$\varepsilon>0,$有：

$$
P(Y\geqslant \varepsilon)\leqslant \frac{E(Y)}{\varepsilon}
$$

证明：

设$Y$的概率密度函数为$f(y),$由期望的定义，结合$Y\geqslant 0,$有：

$$
E(Y)
=\int_{-\infty}^{+\infty}yf(y)\mathrm{d}y
=\int_0^{+\infty}yf(y)\mathrm{d}y
$$

另一方面，

$$
P(Y\geqslant \varepsilon)
=\int_\varepsilon^{+\infty}f(y)\mathrm{d}y
\leqslant \int_\varepsilon^{+\infty}\frac{y}{\varepsilon} f(y)\mathrm{d}y
=\frac{1}{\varepsilon}\int_{\varepsilon}^{+\infty}yf(y)\mathrm{d}y
\leqslant \frac{1}{\varepsilon}\int_{0}^{+\infty}yf(y)\mathrm{d}y
=\frac{E(Y)}{\varepsilon}
$$

也就是说：

$$
P(Y\geqslant \varepsilon)\leqslant \frac{E(Y)}{\varepsilon}
$$

### 切比雪夫不等式

切比雪夫不等式是马尔可夫不等式的一个重要特例

若$D(Y)$存在，则：

$$
P(|Y-E(Y)|\geqslant \varepsilon)\leqslant \frac{D(Y)}{\varepsilon^2}
$$

证明：

由马尔可夫不等式：$P(X\geqslant \varepsilon)\leqslant \frac{E(X)}{\varepsilon} ,$把$X=(Y-E(Y))^2,\varepsilon^2$代替$E,\varepsilon$得到：

$$
P(|Y-E(Y)|\geqslant \varepsilon)\leqslant \frac{D(Y)}{\varepsilon^2}
$$

### 大数定理

设$X_1,\cdots,X_n,\cdots$是独立同分布的随机变量，记它们共同的数学期望为$a,$又设它们的方差存在并记为$\sigma^2.$则对任意给定的$\varepsilon>0,$有：

$$
\lim_{n\to\infty}P(|\bar{X}_n-a|\geqslant \varepsilon)=0
$$

定义频率$\bar{X}_n$为:

$$
\bar{X}_n=\frac{X_1+\cdots+X_n}{n}
$$

证明：

对任意$\varepsilon>0,$

由切比雪夫不等式，有：

$$
P(|\bar{X}_n-E(\bar{X}_n)|\geqslant \varepsilon)
\leqslant \frac{D(\bar{X}_n)}{\varepsilon^2} \tag{1}
$$

注意到：

$$
E(\bar{X}_n)
=E\bigg(\frac{X_1+\cdots+X_n}{n}\bigg)
=\frac{1}{n}E(X_1+\cdots+X_n)
=\frac{1}{n}\bigg(E(X_1)+\cdots+E(X_n)\bigg)
=\frac{1}{n}\cdot\underbrace{(a+\cdots+a)}_{n个a}
=a
$$

又注意到定义：$\bar{X}_n=\frac{X_1+\cdots+X_n}{n},$而$X_1,\cdots,X_n$相互独立，于是：

$$
D(\bar{X}_n)
=\frac{1}{n^2}(D(x_1)+\cdots+D(X_n))
=\frac{1}{n^2}(\sigma^2+\cdots+\sigma^2)
=\frac{\sigma^2}{n}
$$

把上面两式代入(1)，得到：

$$
P(|\bar{X}_n-a|\geqslant \varepsilon)
\leqslant \frac{\sigma^2}{n\varepsilon^2}
$$

取极限，得：

$$
\lim_{n\to\infty}P(|\bar{X}_n-a|\geqslant \varepsilon)
\leqslant \lim_{n\to\infty}\frac{\sigma^2}{n\varepsilon^2}
=0
$$

又由概率性质，概率恒大于等于零，于是由夹逼定理：

$$
\lim_{n\to\infty}P(|\bar{X}_n-a|\geqslant\varepsilon)=0
$$

### 伯努利大数定理

伯努利大数定理是大数定理的一个特例，可以描述为“频率收敛于概率”

$\bar{X}_n$是频率，记为$p_n;$概率记为$p,$对任意$\varepsilon>0,$有：

$$
\lim_{n\to\infty}P(|p_n-p|\geqslant \varepsilon)=0
$$


### 中心极限定理



设 $X_1,\cdots,X_n,\cdots$ 为独立同分布的随机变量，$E(X_i)=a,D(X_i)=\sigma^2(0<\sigma<\infty)$，则对任何实数 $x$，有：

$$
\lim_{n\to\infty}P(\frac{1}{\sqrt{n}\sigma}(X_1+\cdots+X_n-na)\leqslant x)=\varPhi(x)
$$

其中，$\varPhi(x)$是标准正态分布$N(0,1)$的分布函数，即：

$$
\varPhi(x)
=\frac{1}{\sqrt{2\pi}}\int_{{-\infty}}^{x}e^{-\frac{t^2}{2}}\mathrm{d}t
$$

中心极限定理已经做了标准化的工作。

### 拉普拉斯定理

设$X_1,\cdots,X_n,\cdots$独立同分布，$X_i$的分布是：

$$
P(X_i=1)=p,P(X_i=0)=1-p~~~(0<p<1)
$$

则对任何实数$x$，有：

$$
\lim_{n\to\infty}P\bigg(\frac{1}{\sqrt{np(1-p)}}(X_1+\cdots+X_n-np)\leqslant x\bigg)
=\varPhi(x)
$$

## 第三章习题

(浙大版5.1):

设第 $i$ 只元件的寿命为 $X_i,$由题意知，$E(X_i)=100 ,$由指数分布的方差公式知，$D(X_i)=100^2,$设 $16$ 只元件的总寿命为: $X=\sum\limits_{i=1}^{16}X_i.$由中心极限定理：

$$
\frac{X-E(X)}{\sqrt{D(X)}}\mathop{\sim}\limits^{近似地}N(0,1)
$$

即：

$$
\frac{X-16\times 100}{\sqrt{16\times 100^2}}\sim N(0,1)
$$

于是：

$$
P(X>1920)
=1-P(X\leqslant 1920)
=1-P(\frac{X-16\times 100}{\sqrt{16\times 100^2}}\leqslant \frac{1920-16\times 100}{\sqrt{16\times 100^2}})
=1-\varPhi(0.8)
\approx 0.2119
$$

(浙大版5.14)

(1):

设 $X_i=\begin{cases}1,若此人被治愈 \\ 0,若此人没被治愈 \end{cases} ，$记 $X=X_1+X_2+\cdots+X_{100},$ 由题意，$E(X_i)=0.8,D(X_i)=E(X_i^2)-E^2(X_i)=0.8-0.64=0.16$

$$
P(X>75)
=1-P(X\leqslant 75)
=1-P(\frac{X-100\cdot 0.8}{\sqrt{100\cdot 0.16}}\leqslant \frac{75-100\cdot 0.8}{\sqrt{100\cdot 0.16}})
\approx 1-\varPhi(-1.25)
\approx 0.8944
$$

(2):

$E(X_i)=0.7,D(X_i)=0.7-0.49=0.21 $

$$
P(X>75)
=1-P(\frac{X-100\cdot 0.7 }{\sqrt{100\cdot 0.21}}\leqslant \frac{75-100\cdot 0.7}{\sqrt{100\cdot 0.21}})
\approx 1-\varPhi(\frac{5}{\sqrt{21}})
\approx 0.1379
$$


# 样本及抽样分布

总体：试验的全部可能的观察值

个体：每一个可能的观察值

容量：总体中所包含的个体的个数

有限总体：容量有限

无限总体：容量无限

样本：在数理统计中，人们总是从总体中抽出一部分个体，根据获得的数据来对总体分布作出推断的.被抽出的部分个体叫作总体的一个样本

所谓从总体抽取一个个体，就是对总体 $X$ 进行一次观察并记录其结果.

当 $n$ 次观察一经完成，我们就得到一组实数 $x_1,x_2,\cdots,x_n$ 它们依次是随机变量 $X_1,X_2\cdots,X_n$ 的观察值，称为样本值



直方图：

频率直方图：

横轴：组限，组距记为 $\varDelta$

纵轴：$\frac{f_i}{n}/\varDelta $，/$f_i$ 是落在第 $i$ 个区间内的数据频数，$n$ 是样本容量，$\varDelta$ 是组距


### 样本分位数

设有容量为 $n$ 的样本观察值 $x_1,x_2,\cdots,x_n, $样本 $p$ 分位数($0<p<1 $)记为 $x_p$ ，它具有以下性质：

(1)：至少有 $np$ 个观察值小于或等于 $x_p.$

(2)：至少有 $n(1-p)$ 个观察值大于或等于 $x_p$

$p$ 分位数 $x_p$ 计算公式：

$$
x_p=
\begin{cases}
x_{([np]+1)} &,当np不是整数 \\
\frac{1}{2}(x_{(np)}+x_{(np+1)}) &,当np是整数
\end{cases}
$$

$0.25$ 分位数 $x_{0.25}$ 称为第一四分位数，又记为 $Q_1;$

$0.5$ 分位数 $x_{0.5}$ 称为样本中位数，又记为 $Q_2$ 或 $M;$

$0.75$ 分位数 $x_{0.75}$ 称为第三四分位数，又记为 $Q_3$

### 箱线图

(1) 画一水平数轴，在轴标上 $\min,Q_1,M,Q_3,\max,$ 在数轴上方画一个上、下侧平行于数轴的矩形箱子，箱子左右两侧分别位于 $Q_1,Q_3$ 的上方. 在 $M$ 点的上方画一条垂直线段，线段位于箱子内部.

(2) 从箱子左侧引一条水平线至最小值 $\min$，在同一水平高度从箱子右侧引一条水平线至 $\max.$ 

这样箱线图就作好了.

疑似异常值：若在数据集中某一个观察值不寻常地大于或小于该数集中地其他数据，则称之为**疑似异常值**.

第一四分位数 $Q_1$ 与第三四分位数 $Q_3$ 之间地距离 $Q_3-Q_1\mathop{=}\limits^{记成} IQR, $ 称为**四分位间距数**

若某个数据小于 $Q_1-1.5IQR$ 或大于 $Q_3+1.5IQR,$ 就认为它是疑似异常值.

修正箱线图：

(1) 画一水平数轴，在轴标上 $\min,Q_1,M,Q_3,\max,$ 在数轴上方画一个上、下侧平行于数轴的矩形箱子，箱子左右两侧分别位于 $Q_1,Q_3$ 的上方. 在 $M$ 点的上方画一条垂直线段，线段位于箱子内部.

(2) 计算 $IQR=Q_3-Q_1.$ 若某个数据小于 $Q_1-1.5IQR$ 或大于 $Q_3+1.5IQR,$ 则认为它是一个疑似异常值. 画出疑似异常值，并以 * 表示.

(3) 从箱子左侧引一条水平线至数据集中除去疑似异常值后的最小值，再在同一水平高度从箱子右侧引一水平线至数据集中除去疑似异常值后的最大值.


## 抽样分布

设 $X_1,X_2,\cdots,X_n$ 是来自总体 $X$ 的一个样本，$g(X_1,X_2,\cdots,X_n)$ 是 $X_1,X_2,\cdots,X_n$ 的函数，若 $g$ 中不含未知参数，则称 $g(X_1,X_2,\cdots,X_n)$ 是一统计量.

显然，统计量也是一个随机变量.

设 $x_1,x_2,\cdots,x_n$ 是相应于 $X_1,X_2,\cdots,X_n$ 的样本值，则称 $g(x_1,x_2,\cdots,x_n)$ 是 $g(X_1,X_2,\cdots,X_n)$ 的观察值.

### 几个常用的统计量

样本均值：

$$
\bar{X}
\equiv \frac{1}{n}\sum_{i=1}^{n}X_{i}
$$

样本方差：

$$
S^2
\equiv \frac{1}{n-1}\sum_{i=1}^{n}(X_i-\bar{X})^2 
=\frac{1}{n-1}(-n\bar{X}^2+\sum_{i=1}^{n} X_i^2 )
$$

样本标准差：

$$
S
\equiv \sqrt{S^2}
\equiv \sqrt{\frac{1}{n-1}\sum_{i=1}^{n}(X_i-\bar{X})^2}
$$

样本 $k$ 阶(原点)矩：

$$
A_k
\equiv \frac{1}{n}\sum_{i=1}^{n}X_i^k,~~~k=1,2,\cdots
$$

样本 $k$ 阶中心矩：

$$
B_k
\equiv \frac{1}{n}\sum_{i=1}^{n}(X_i-\bar{X})^k,~~~k=2,3,\cdots
$$

它们的观察值分别为：

$$
\bar{x}
\equiv \frac{1}{n}\sum_{i=1}^{n} x_i
$$

$$
s^2\equiv \frac{1}{n-1}\sum_{i=1}^{n} (x_i-\bar{x})^2
=\frac{1}{n-1}(-n\bar{x}^2+\sum_{i=1}^{n} x_i^2)
$$

$$
s
\equiv \sqrt{\frac{1}{n-1}\sum_{i=1}^{n}(x_i-\bar{x})^2 }
$$

$$
a_k=\frac{1}{n}\sum_{i=1}^{n}x_i^k,~~~k=1,2,\cdots
$$

$$
b_k=\frac{1}{n}\sum_{i=1}^{n}(x_i-\bar{x})^k,~~~k=2,3,\cdots
$$



样本方差 $S^2=\frac{1}{n-1}(-n\bar{X}^2+\sum\limits_{i=1}^n X_i^2) $的推导： 

$$
\begin{aligned}
S^2
&\equiv \frac{1}{n-1}\sum_{i=1}^{n}(X_i-\bar{X})^2 \\
&=\frac{1}{n-1}\sum_{i=1}^{n} (X_i^2-2\bar{X}X_i+\bar{X}^2) \\
&=\frac{1}{n-1}(\sum_{i=1}^{n} X_i^2+\sum_{i=1}^{n} -2\bar{X}X_i+\sum_{i=1}^{n} \bar{X}^2 ) \\
&=\frac{1}{n-1}(\sum_{i=1}^{n} X_i^2-2\bar{X}\sum_{i=1}^{n} X_i+\sum_{i=1}^{n} \bar{X}^2) \\
&=\frac{1}{n-1}\bigg((\sum_{i=1}X_i^2) -2n\bar{X}^2+n\bar{X}^2\bigg ) \\
&=\frac{1}{n-1}(-n\bar{X}^2+\sum_{i=1}^{n}X_i^2)
\end{aligned}
$$

### $\chi^2$ 分布：

设 $X_1,X_2,\cdots,X_n$ 是来自总体 $N(0,1)$ 的样本，则称统计量：

$$
\chi^2=X_1^2+X_2^2+\cdots+X_n^2
$$

服从自由度为 $n$ 的$\chi^2$ 分布，记为：$\chi^2\sim \chi^2(n)$

$\chi^2(n)$ 的概率密度为：

$$
f(y)
=
\begin{cases}
0 &,y\leqslant 0 \\
\frac{1}{2^{\frac{n}{2}}\Gamma(\frac{n}{2}) }y^{\frac{n}{2}-1}e^{-\frac{y}{2}} &,y>0
\end{cases}
$$

$\chi^2 $ 分布的可加性：

设 $\chi_1^2\sim \chi^2(n_1),\chi_2^2\sim \chi^2(n_2),$并且 $\chi_1^2,\chi_2^2$ 相互独立，则有：

$$
\chi_1^2+\chi_2^2\sim \chi^2(n_1+n_2)
$$

$\chi^2$ 分布的数学期望和方差：

若 $\chi^2\sim \chi^2(n),$则：

$$
E(\chi^2)=n
$$

$$
D(\chi^2)=2n
$$

证明：

设 $\chi^2\sim \chi^2(n),$ 即：

$$
\chi^2=X_1^2+X_2^2+\cdots+X_n^2
$$

其中，$X_i\sim N(0,1),(i=1,2,\cdots,n)$

$$
E(X_i)=0
$$

$$
D(X_i)=1
$$

$$
E(X_i^2)
=D(X_i)+E^2(X_i)
=1
$$

$$
D(X_i^2)
=E(X_i^4)-E^2(X_i^2)
=3-1
=2
$$

$$
E(\chi^2)
=E(X_1^2+X_2^2+\cdots+X_n^2)
=\sum_{i=1}^{n}E(X_i^2)
=n
$$

$$
D(\chi^2)
=D(X_1^2+X_2^2+\cdots+X_n^2)
=2n
$$

$\chi^2$ 分布的上分位数:

对于给定的正数 $\alpha,0<\alpha<1,$满足条件：

$$
P(\chi^2>\chi_\alpha^2(n))
=\int_{\chi_\alpha^2(n)}^{+\infty}f(y)\mathrm{d}y
=\alpha
$$

的 $\chi_\alpha^2(n)$就是 $\chi^2(n)$ 分布的上 $\alpha$ 分位数

当 $n$ 充分大时，近似地有：

$$
\chi_\alpha^2(n)
=\frac{1}{2}(z_\alpha+\sqrt{2n-1})^2
$$

其中，$z_\alpha$ 是标准正态分布的上 $\alpha$ 分位数.

### $t$ 分布

设 $X\sim N(0,1),Y\sim \chi^2(n) $且 $X,Y$ 相互独立，则称随机变量：

$$
t=\frac{X}{\sqrt{\frac{Y}{n}}}
$$

服从自由度为 $n$ 的 $t$ 分布，记为：$t\sim t(n)$

$t(n)$ 分布的概率密度函数为：

$$
h(t)
=\frac{\Gamma(\frac{n+1}{2})}{\sqrt{\pi n}\Gamma(\frac{n}{2})}(1+\frac{t^2}{n})^{-\frac{n+1}{2}},~~~-\infty<t<+\infty
$$

$h(t)$ 的图像关于 $t=0$ 对称，当 $n$ 充分大时其图像类似于标准正态分布变量概率密度的图像.

当 $n$ 足够大时，$t$ 分布近似于 $N(0,1)$ 分布. 但对较小的 $n$, $t$ 分布与 $N(0,1)$ 分布有较大差别.

$t$ 分布的上分位数：

对于给定的 $\alpha,0<\alpha<1,$满足条件：

$$
P(t>t_\alpha(n))
=\int_{t_\alpha(n)}^{+\infty} h(t)\mathrm{d}t
=\alpha
$$

的 $t_\alpha(n)$ 就是 $t(n)$ 分布的上 $\alpha$ 分位数.

由 $t(n)$ 分布的上 $\alpha$ 分位数的定义及 $h(t)$ 图像的对称性知：

$$
t_\alpha(n)+t_{1-\alpha}(n)
=0
$$

当 $n>45$时，对于常用的 $\alpha$ 值，就用正态分布近似：

$$
t_\alpha(n)\approx z_\alpha
$$


### $F$ 分布

设 $U\sim \chi^2(n_1),V\sim \chi^2(n_2),$且 $U,V$ 相互独立，则称随机变量：

$$
F=\frac{\frac{U}{n_1}}{\frac{V}{n_2}}
$$

服从自由度为 $(n_1,n_2)$ 的 $F$ 分布，记为：$F\sim F(n_1,n_2)$

$F(n_1,n_2)$ 分布的概率密度为：

$$
\psi(y)
=\begin{cases}
0&,y\leqslant 0 \\
\frac{\Gamma(\frac{n_1+n_2}{2})(\frac{n_1}{n_2})^\frac{n_1}{2}y^{\frac{n_1}{2}-1}}{\Gamma(\frac{n_1}{2})\Gamma(\frac{n_2}{2})(1+\frac{n_1 }{n_2}y)^{\frac{n_1+n_2}{2}}}&,y>0
\end{cases}
$$

若 $F\sim F(n_1,n_2),$则：

$$
\frac{1}{F}
\sim F(n_2,n_1)
$$

$F$ 分布的上分位数：

对于给定的 $\alpha,0<\alpha<1,$满足条件：

$$
P(F>F_\alpha(n_1,n_2))
=\int_{F_\alpha(n_1,n_2)}^{+\infty} \psi(y)\mathrm{d}y
=\alpha
$$

的 $\alpha$ 就是 $F(n_1,n_2)$ 分布的上 $\alpha$ 分位数.

$F$ 分布的上 $\alpha$ 分位数的性质：

$$
F_\alpha(n_1,n_2)
=\frac{1}{F_{1-\alpha}(n_2,n_1)}
$$

证明：

设 $F\sim F(n_1,n_2),$ 则 $\frac{1}{F}\sim F(n_2,n_1)$.$0<\alpha<1$,

$$
\begin{aligned}
\alpha
&=P(F>F_{\alpha}(n_1,n_2)) \\
\end{aligned}
$$

而由于 $\frac{1}{F}\sim F(n_2,n_1),$ 于是：

$$
1-\alpha=P(\frac{1}{F}>F_{1-\alpha}(n_2,n_1) )
$$

两式相加，得：

$$
1=
P(F>F_\alpha(n_1,n_2))+P(\frac{1}{F}>F_{1-\alpha}(n_2,n_1))
$$

接着：

$$
1-P(F>F_\alpha(n_1,n_2))=P(F<\frac{1}{F_{1-\alpha}(n_2,n_1)})
$$

于是：

$$
P(F\leqslant F_\alpha(n_1,n_2))
=P(F<\frac{1}{F_{1-\alpha}(n_2,n_1)})
$$

而 $P(F\leqslant F_\alpha(n_1,n_2))=P(F< F_\alpha(n_1,n_2))$

于是：

$$
P(F< F_\alpha(n_1,n_2))
=P(F<\frac{1}{F_{1-\alpha}(n_2,n_1)})
$$

概率分布函数的单调性知：

$$
F_\alpha(n_1,n_2)
=\frac{1}{F_{1-\alpha}(n_2,n_1)}
$$

### 正态总体的样本均值与样本方差的分布



总体 $X$ 的数学期望 $E(X)$ 记为 $\mu,$ 方差 $D(X)$ 记为 $\sigma^2,$ $X_1,X_2\cdots,X_n$ 是来自 $X$ 的一个样本， $\bar{X},S^2 $ 分别是样本均值和样本方差，则有：

$$
E(\bar{X})=\mu
$$

$$
D(\bar{X})=\frac{\sigma^2}{n}
$$

这就是说，样本均值的数学期望等于样本的数学期望；样本均值的方差

注意到(第二个等号的 导出用到了样本标准差定义的导出结果)：

$$
\begin{aligned}
E(S^2)
&=E(\frac{1}{n-1}\sum_{i=1}^{n} (X_i-\bar{X})^2) \\
&=\frac{1}{n-1}E(-n\bar{X}^2+\sum_{i=1}^{n}X_i^2 ) \\
&=\frac{1}{n-1}(-nE(\bar{X^2})+\sum_{i=1}^{n}E(X_i^2)) \\
&=\frac{1}{n-1}\bigg(-n\big(D(\bar{X})+E^2(\bar{X}) \big)+\sum_{i=1}^{n} \big(D(X_i)+E^2(X_i) \big) \bigg) \\
&=\frac{1}{n-1}\bigg(-n(\frac{\sigma^2}{n}+\mu^2)+\sum_{i=1}^{n} (\sigma^2+\mu^2) \bigg) \\
&=\sigma^2
\end{aligned}
$$

这说明，样本方差的数学期望等于总体的方差

定理：

设 $X_1,X_2,\cdots,X_n$ 是来自正态总体 $N(\mu,\sigma^2)$ 的样本，$\bar{X}$ 是样本的均值，则有：

$$
\bar{X}
\sim N(\mu,\frac{\sigma^2}{n})
$$

证明：

由于几个相互独立且服从同一正态分布的随机变量的线性组合仍然服从正态分布，为得到此正态分布的参数，我们计算：

$$
E(\bar{X})
=\mu 
$$

$$
D(\bar{X})
=\frac{\sigma^2}{n}
$$

于是：

$$
\bar{X}\sim N(\mu,\frac{\sigma^2}{n})
$$

定理：

设 $X_1,X_2,\cdots,X_n$ 是来自总体 $N(\mu,\sigma^2)$ 的样本，$\bar{X},S^2$ 分别是样本均值和样本方差，则有：

1. $\frac{(n-1)S^2}{\sigma^2}\sim \chi^2(n-1) $

2. $\bar{X}$ 与 $S^2$ 相互独立.

证明：

定理：

设 $X_1,X_2,\cdot,X_n$ 是来自总体 $N(\mu,\sigma^2)$ 的样本，$\bar{X},S^2$ 分别是样本均值和样本方差，则有：

$$
\frac{\bar{X}-\mu}{\frac{S}{\sqrt{n}}}
\sim t(n-1)
$$

证明：

定理：

设 $X_1,X_2,\cdots,X_{n_1}$ 和 $Y_1,Y_2,\cdots,Y_{n_2}$ 分别是来自正态总体 $N(\mu_1,\sigma_1^2)$ 和 $N(\mu_2,\sigma_2^2)$ 的样本，且这两个样本相互独立. 设 $\bar{X}=\frac{1}{n}\sum\limits_{i=1}^{n_1}X_{i},\bar{Y}=\frac{1}{n}\sum\limits_{i=1}^{n_2}Y_i, $ 是这两个样本的样本均值；$S_1^2=\frac{1}{n_1-1}\sum\limits_{i=1}^{n_1}(X_i-\bar{X})^2,S_2^2=\frac{1}{n_2-1}\sum\limits_{i=1}^{n_2} (Y_i-\bar{Y})^2 $ 分别是这两个样本的样本方差，则有：

1.

$$
\frac{\frac{S_1^2}{S_2^2}}{\frac{\sigma_1^2}{\sigma_2^2}}
\sim F(n_1-1,n_2-1)
$$

2.

当 $\sigma_1^2=\sigma_2^2=\sigma^2$ 时，

$$
\frac{(\bar{X}-\bar{Y})-(\mu_1-\mu_2)}{S_W\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}}
\sim t(n_1+n_2-2)
$$

其中，

$$
S_W^2
=\frac{(n_1-1)S_1^2+(n_2-1)S_2^2}{n_1+n_2-2}
$$

证明：

## 习题

（浙大版6.4）

（1）：

解：

若 $CY$ 服从 $\chi^2$ 分布，则 $\sqrt{C}(X_1+X_2+X_3)\sim N(0,1) ,\sqrt{C}(X_4+X_5+X_6)\sim N(0,1) $ 

$$
D(\sqrt{C}(X_1+X_2+X_3))
=1
$$

即：

$$
3C
=1
$$

于是 $C=\frac{1}{3}$

（2）：

若 $Y$ 服从 $t$ 分布，由于 $Y$ 可以写成：

$$
Y
=\frac{\frac{C}{\sqrt{3}}(X_1+X_2)}{\sqrt{\frac{X_1^2+X_2^2+X_3^2}{3}}}
$$

分母已经是 $t$ 分布分母的形式，所以只要满足：$\frac{C}{\sqrt{3}}(X_1+X_2)\sim N(0,1) .$ 故应有：

$$
D(\frac{C}{\sqrt{3}}(X_1+X_2))=1
$$

即：

$$
\frac{C^2}{3}\cdot 2
=1
$$

解得：

$$
C=\pm\frac{\sqrt{6}}{2}
$$

（3）：

解：

已知 $X\sim t(n),$ 则 $X$ 应有如下形式：

$$
X
=\frac{Z}{\sqrt{\frac{Y}{n}}}
$$

其中，$Z\sim N(0,1),Y\sim \chi^2(n)$

注意到：

$$
X^2
=\frac{Z^2}{\frac{Y}{n}}
=\frac{\frac{Z^2}{1}}{\frac{Y}{n}}
$$

注意到，$Z\sim N(0,1),$ 则 $Z^\sim \chi^2(1)$

于是由 $F$ 分布的定义，有：

$$
X^2\sim F(1,n)
$$

（浙大版6.9）

（1）：

由定理，

$$
\frac{15 S^2}{\sigma^2}\sim t(15)
$$

于是：

$$
P(\frac{S^2}{\sigma^2}\leqslant 2.041)
=P(\frac{15S^2}{\sigma^2}\leqslant 30.615)
=1-P(\frac{15S^2}{\sigma^2}>30.615)
$$

查表知，当 $n=15,$ 有 $\chi^2_{0.01}(15)\approx 30.577$ 由于 $30.577\approx 30.615,$ 于是可近似认为：

$$
P(\frac{15S^2}{\sigma^2}>30.615)
\approx P(\frac{15S^2}{\sigma^2}>30.577)
\approx 0.01 
$$

于是：

$$
P(\frac{S^2}{\sigma^2}\leqslant 2.041)
\approx 0.99
$$



# 参数估计

## 点估计

总体 $X$ 的分布函数已知，但它的一个或多个参数未知，借助于总体 $X$ 的一个样本来估计总体未知参数的值的问题称为参数的**点估计**问题.




### 矩估计法

设 $X$ 为连续型随机变量，其概率密度为：$f(x;\theta_1,\theta_2,\cdots,\theta_k) ,$ 或 $X$ 为离散型随机变量，其分布律为：$P(X=x)=p(x;\theta_1,\theta_2,\cdots,\theta_k) $.其中，$\theta_1,\theta_2,\cdots,\theta_k$ 为待估参数，$X_1,X_2,\cdots,X_n $ 是来自 $X$ 的样本，假设总体 $X$ 的前 $k$ 阶矩：

$$
\mu_l
=E(X^l)

=\int_{-\infty}^{+\infty} x^l f(x;\theta_1,\theta_2,\cdots,\theta_k) \mathrm{d}x~~~(X为连续型)
$$

或

$$
\mu_l
=E(X^l)
=\sum_{x\in R_X} x^l p(x;\theta_1,\theta_2,\cdots,\theta_k)~~~(X为离散型)
$$

存在，$l=1,2,\cdots,k.$ 一般来说，它们是 $\theta_1,\theta_2,\cdots,\theta_k$ 的函数.

其中 $R_X$ 是 $X$ 可能取值的范围.

基于样本矩：

$$
A_l
=\frac{1}{n}\sum_{i=1}^{n} X_i^l
$$

依概率收敛于相应的总体矩 $\mu_l(l=1,2,\cdots,k),$ 样本矩的连续函数依概率收敛于相应的总体矩的连续函数.于是我们就用样本矩作为相应的总体矩的估计量，而以样本矩的连续函数作为相应的总体矩的连续函数的估计量. 这种估计方法称为**矩估计法**.

矩估计法的具体做法：

设：

$$

\begin{cases}

\mu_1=\mu_1(\theta_1,\theta_2,\cdots,\theta_k) \\
\mu_2=\mu_2(\theta_1,\theta_2,\cdots,\theta_k) \\
~~~~~~\vdots \\
\mu_k=\mu_k(\theta_1,\theta_2,\cdots,\theta_k)

\end{cases}

$$

其中，所有方程等式左边的 $\mu_i(\theta_1\,\theta_2,\cdots,\theta_k)$ 是某个关于 $\theta_1,\theta_2,\cdots,\theta_k$ 的表达式.(意思有点像 $y=y(x)$)

其中可以解出：

$$
\begin{cases}
\theta_1=\theta_1(\mu_1,\mu_2,\cdots,\mu_k) \\
\theta_2=\theta_2(\mu_1,\mu_2,\cdots,\mu_k) \\
~~~~~~\vdots \\
\theta_k=\theta_k(\mu_1,\mu_2,\cdots,\mu_k) 
\end{cases}
$$

以 $A_i$ 分别代替上式中的 $\mu_i,(i=1,2,\cdots,k),$ 就以

$$
\hat{\theta}_i
=\theta_i(A_1,A_2\cdots,A_k),~~~i=1,2,\cdots,k
$$

分别作为 $\theta_i,i=1,2,\cdots,k$ 的估计量，这种估计量称为矩估计量. 矩估计量的 观察值称为矩估计值.

结论：总体均值与方差的矩估计量的表达式不因不同的总体分布而异

$$
\hat{\mu}=\bar{X}
$$

$$
\hat{\sigma^2}=\frac{1}{n}\sum_{i=1}^{n}(X_i-\bar{X})^2
$$




### 最大似然估计法

若总体 $X$ 属离散型，其分布律 $P(X=x)=p(x;\theta),\theta\in \varTheta $ 的形式为已知，$\varTheta $ 是 $\theta$ 可能取值的范围. 设 $X_1,X_2,\cdots,X_n$ 是来自 $X_n$ 的样本，则 $X_1,X_2,\cdots,X_n$ 的联合分布律为：

$$
\prod_{i=1}^n p(x_i;\theta)
$$

又设 $x_1,x_2,\cdots,x_n$ 是相应于样本 $X_1,X_2,\cdots,X_n$ 的一个样本值，则样本 $X_1,X_2,\cdots,X_n $ 取到观察值 $x_1,x_2,\cdots,x_n$ 的概率，亦即事件 $\{X_1=x_1,X_2=x_2,\cdots,X_n=x_n \} $ 的概率为：

$$
L(\theta)
=L(x_1,x_2,\cdots,x_n;\theta) 
=\prod_{i=1}^n p(x_i;\theta),~~~\theta\in\varTheta
$$ 

这一概率随 $\theta$ 的取值的变化而变化，它是 $\theta $ 的函数，$L(\theta)$ 称为样本的似然函数.

在 $\theta$ 可能的取值 $\varTheta$ 内选择使得似然函数 $L(x_1,x_2,\cdots,x_n;\theta)$ 达到**最大**的参数值 $\hat{\theta}$ 作为 $\theta$ 的估计值. 即取 $\hat{\theta} $ 使得：

$$
L(x_1,x_2,\cdots,x_n;\hat{\theta})
=\max_{\theta\in \varTheta}L(x_1,x_2,\cdots,x_n;\theta)
$$

这样得到的 $\hat{\theta} $ 与样本值 $x_1,x_2,\cdots,x_n $ 有关，常记为 $\hat{\theta}(x_1,x_2,\cdots,x_n) ,$ 称为参数 $\theta$ 的**最大似然估计值**，而相应的统计量 $\hat{\theta}(X_1,X_2,\cdots,X_n) $ 称为参数 $\theta$ 的**最大似然估计量**

若总体 $X$ 属连续型，其概率密度 $f(x;\theta),\theta\in \varTheta$ 的形式已知，$\theta$ 为待估参数，$\varTheta$ 是 $\theta$ 可能取值的范围. 设 $X_1,X_2,\cdots,X_n$ 是来自 $X$ 的样本，则 $X_1,X_2,\cdots,X_n$ 的联合分布概率密度为：

$$
\prod_{i=1}^{n}f(x_i ;\theta)
$$

设 $x_1,x_2,\cdots,x_n$ 是相应于 $X_1,X_2,\cdots,X_n$ 的一个样本值，则随机点 $(X_1,X_2,\cdots,X_n) $ 落在点 $(x_1,x_2,\cdots,x_n) $ 的邻域内概率近似为：

$$
\prod_{i=1}^{n} f(x_i;\theta)\mathrm{d}x_i
$$

我们取 $\theta$ 的估计值 $\hat{\theta}$ 使上面的概率取到最大，但由于因子 $\prod\limits_{i=1}^{n}\mathrm{d}x_i $ 不随 $\theta$ 而变，故只需要考虑函数：

$$
L(\theta)
=L(x_1,x_2,\cdots,x_n;\theta)
=\prod_{i=1}^{n}f(x_i;\theta)
$$

的最大值. $L(\theta)$ 称为样本的**似然函数**.

若：

$$
=L(x_1,x_2,\cdots,x_n;\hat{\theta})
=\max_{\theta\in\varTheta}L(x_1,x_2,\cdots,x_n;\theta)
$$

则称 $\hat{\theta}(x_1,x_2,\cdots,x_n) $ 为 $\theta$ 的**最大似然估计值**，称 $\hat{\theta}(X_1,X_2,\cdots,X_n) $ 为 $\theta$ 的**最大似然估计量**.

$\hat{\theta}$ 可从方程：

$$
\frac{\mathrm{d}}{\mathrm{d}\theta}L(\theta)=0
$$

中解得. 或从方程：

$$
\frac{\mathrm{d}}{\mathrm{d}\theta}\ln L(\theta)=0
$$

中解得.

例：

设 $X\sim B(1,p).X_1,X_2,\cdots,X_n$ 是来自 $X$ 的一个样本. 试求参数 $p$ 的最大似然估计量.

答案：$\hat{p}=\bar{X} $


例：

设总体 $X\sim N(\mu,\sigma^2),\mu,\sigma^2$ 为未知参数，$x_1,x_2,\cdots,x_n$ 是来自 $X$ 的一个样本值，求 $\mu,\sigma^2$ 的最大似然估计量.

答案：$\hat{\mu}=\bar{X},\hat{\sigma^2}=\frac{1}{n}\sum\limits_{i=1}^{n} (X_i-\bar{X})^2 $

例：

设总体 $X$ 在 $[a,b] $ 上服从均匀分布，$a,b$ 未知，$x_1,x_2,\cdots,x_n$ 是一个样本值. 试求 $a,b$ 的最大似然估计量.

答案：$\hat{a}=\min\limits_{1\leqslant i\leqslant n}X_i,\hat{b}=\max\limits_{1\leqslant i\leqslant n}X_i $



## 估计量的评选标准

### 无偏性

设 $X_1,X_2,\cdots,X_n$ 是总体 $X$ 的一个样本，$\theta\in\varTheta $ 是包含在总体 $X$ 的分布中的待估参数，这里 $\varTheta$ 是 $\theta$ 的取值范围.

若估计量 $\hat{\theta}=\hat\theta(X_1,X_2,\cdots,X_n) $ 的数学期望 $E(\hat{\theta}) $ 存在，且对于任意 $\theta\in\varTheta$ 有：

$$
E(\hat{\theta})=\theta
$$

则称 $\hat{\theta}$ 是 $\theta$ 的**无偏估计量**.

系统误差：$E(\hat{\theta})-\theta $


### 有效性

设 $\hat{\theta}_1=\hat{\theta}_1(X_1,X_2,\cdots,X_n) $ 与 $ \hat{\theta}_2=\hat{\theta}_2 (X_1,X_2,\cdots,X_n) $ 都是 $\theta$ 的无偏估计量，若对于任意 $\theta\in\varTheta$ 有：

$$
D(\hat{\theta}_1)\leqslant D(\hat{\theta}_2)
$$

且至少对于某一个 $\theta\in\varTheta$ 上式中的不等号成立，则称 $\hat{\theta}_1$ 较 $\hat{\theta}_2$ 有效. 

### 相合性

设 $\hat{\theta}(X_1,X_2,\cdots,X_n) $ 为参数 $\theta$ 的估计量，若对于任意 $\theta\in\varTheta,$ 当 $n\to \infty$ 时 $\hat{\theta}(X_1,X_2,\cdots,X_n) $ 依概率收敛于 $\theta, $ 则称 $\hat{\theta}$ 为$\theta$ 的相合估计量


### 区间估计

置信区间(字母下面一横咋打嘛？)：

设总体 $X$ 的分布函数 $F(x;\theta)$ 含有一个未知参数 $\theta\in\varTheta,$ $\varTheta$ 是 $\theta$ 可能取值的范围，对于给定值 $\alpha(0<\alpha<1),$ 若来自 $X$ 的样本 $X_1,X_2,\cdots,X_n$ 确定的两个统计量 $\theta_1=\theta_1(X_1,X_2,\cdots,X_n)$ 和 $\theta_2=\theta_2(X_1,X_2,\cdots,X_n),\theta_1<\theta_2,$ 对于任意 $\theta\in \varTheta$ 满足：

$$
P(\theta_1(X_1,X_2,\cdots,X_n)<\theta<\theta_2(X_1,X_2,\cdots,X_n))\geqslant 1-\alpha
$$

则称随机区间 $(\theta_1,\theta_2)$ 是 $\theta$ 的置信水平为 $1-\alpha$ 的置信区间，$\theta_1,\theta_2$ 分别称为置信水平为 $1-\alpha$ 的双侧置信区间的置信下限和置信上限，$1-\alpha$ 称为置信水平.

寻找未知参数 $\theta$ 的置信区间的具体做法：

1.寻求一个样本 $X_1,X_2,\cdots,X_n$ 和 $\theta$ 的函数，$W=W(X_1,X_2,\cdots,X_n;\theta)$ ，使得 $W$ 的分布不依赖于 $\theta$ 以及其他未知参数，称具有这种性质的函数 $W$ 为**枢轴量**.

2.对于给定的置信水平 $1-\alpha$,定出两个常数 $a,b$ 使得：

$$
P(a<W(X_1,X_2,\cdots,X_n;\theta)<b)=1-\alpha
$$

若能从 $a<W(X_1,X_2,\cdots,X_n;\theta)<b$ 得到与之等价的关于 $\theta$ 的不等式 $\theta_1<\theta<\theta_2$，其中 $\theta_1=\theta_1(X_1,X_2,\cdots,X_n),\theta_2=\theta_2(X_1,X_2,\cdots,X_n)$ 都是统计量，那么 $(\theta_1,\theta_2)$ 就是 $\theta$ 的一个置信水平为 $1-\alpha$ 的置信区间.

####  单个总体 $N(\mu,\sigma^2)$

设已给定置信水平为 $1-\alpha, $ 并设 $X_1,X_2,\cdots,X_n$ 为总体 $N(\mu,\sigma^2)$ 的样本，$\bar{X},S^2 $ 分是样本均值和样本方差.

1.均值 $\mu$ 的置信区间：

(1) $\sigma^2$ 为已知

构造枢轴量为： 
$$
\frac{\bar{X}-\mu}{\frac{\sigma}{\sqrt{n}}} 
\sim N(0,1)
$$

$$
P(\bigg|\frac{\bar{X}-\mu}{\frac{\sigma}{\sqrt{n}}}\bigg|\leqslant z_{\frac{\alpha}{2}})=1-\alpha
$$



$\mu$ 的一个置信水平为 $1-\alpha$ 的置信区间为：

$$
(\bar{X}- \frac{\sigma}{\sqrt{n}}z_{\frac{\alpha}{2}},\bar{X}+\frac{\sigma}{\sqrt{n}}z_{\frac{\alpha}{2}})
$$


(2) $\sigma^2$ 为未知

构造枢轴量为：

$$
\frac{\bar{X}-\mu}{\frac{S}{\sqrt{n}}}\sim t(n-1)
$$

$\mu$ 的一个置信水平为 $1-\alpha$ 的置信区间为：

$$
\bigg(\bar{X}- \frac{S}{\sqrt{n}}t_{\frac{\alpha}{2}}(n-1),\bar{X}+\frac{S}{\sqrt{n}}t_{\frac{\alpha}{2}}(n-1)\bigg)
$$

2.方差 $\sigma^2$ 的置信区间

构造枢轴量为：

$$
\frac{(n-1)S^2}{\sigma^2}
\sim \chi^2(n-1)
$$

方差 $\sigma^2$ 的一个置信水平为 $1-\alpha$ 的置信区间为：

$$
(\frac{(n-1)S^2}{\chi_{\frac{\alpha}{2}}^2(n-1)},\frac{(n-1)S^2}{\chi_{1- \frac{\alpha}{2}}^2(n-1)})
$$

两个总体 $N(\mu_1,\sigma_1^2),N(\mu_2,\sigma_2^2)$ 的情况

1.两个总体均值差 $\mu_1-\mu_2$ 的置信区间

(1)$\sigma_1^2,\sigma_2^2$ 均为已知

$$
\bar{X}-\bar{Y}
\sim N(\mu_1-\mu_2,\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2})
$$

或构造枢轴量：

$$
\frac{\bar{X}-\bar{Y}-(\mu_1-\mu_2)}{\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}}}
\sim N(0,1)
$$

$\mu_1-\mu_2$的一个置信水平为 $1-\alpha$ 的置信区间为：

$$
(\bar{X}-\bar{Y}\pm z_{\frac{\alpha}{2}}\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}})
$$


(2) $\sigma_1^2=\sigma_2^2=\sigma^2 $ 但 $\sigma^2 $ 为未知

构造枢轴量：

$$
\frac{\bar{X}-\bar{Y}-(\mu_1-\mu_2)}{S_W\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}}
\sim t(n_1+n_2-2)
$$

$\mu_1-\mu_2$ 的一个置信水平为 $1-\alpha$ 的置信区间为：

$$
(\bar{X}-\bar{Y}\pm t_{\frac{\alpha}{2}}(n_1+n_2-2)S_W\sqrt{\frac{1}{n_1}+\frac{1}{n_2}})
$$

2.两个总体方差比 $\frac{\sigma_1^2}{\sigma_2^2}$ 的置信区间

讨论总体均值 $\mu_1,\mu_2$ 均为未知的情况

$$
\frac{S_1^2/S_2^2}{\sigma_1^2 / \sigma_2^2}
\sim F(n_1-1,n_2-1)
$$

$\frac{\sigma_1^2}{\sigma_2^2}$ 的一个置信水平为 $1-\alpha$ 的置信区间为：

$$
(\frac{S_1^2}{S_2^2}\frac{1}{F_{\frac{\alpha}{2}}(n_1-1,n_2-1)},\frac{S_1^2}{S_2^2}\frac{1}{F_{1-\frac{\alpha}{2}}(n_1-1,n_2-1)})
$$

### (0-1) 分布参数的区间估计

设有一容量 $n>50$ 的大样本，它来自(0-1) 分布的总体 $X,$ $X$ 的分布律为：

$$
f(x;p)=p^x(1-p)^{1-x},x=0,1
$$

其中，$p$ 为未知参数，现在来求 $p$ 的置信水平为 $(1-\alpha) $ 的置信区间

$$
(n+z_{\frac{\alpha}{2}}^2)p^2-(2n\bar{X}+z_{\frac{\alpha}{2}}^2)p+n\bar{X}^2<0
$$

令：

$$
a=n+z_{\frac{\alpha}{2}}^2 \\
b=-(2n\bar{X}+z_{\frac{\alpha}{2}}^2) \\
c=n\bar{X}^2
$$

记：

$$
p_1
=\frac{1}{2a}(-b-\sqrt{b^2-4ac})
$$

$$
p_2
=\frac{1}{2a}(-b+\sqrt{b^2-4ac})
$$

则 $p$ 的一个近似的置信水平为 $1-\alpha$ 的置信区间为：

$$
(p_1,p_2)
$$


单侧置信区间

对于给定值 $\alpha\in(0,1),$ 若由样本 $X_1,X_2,\cdots,X_n$ 确定的统计量 $\theta_1=\theta_1(X_1,X_2,\cdots,X_n)$，对于任意的 $\theta\in\varTheta$ 满足：

$$
P(\theta>\theta_1)\geqslant 1-\alpha
$$

则称随机区间 $(\theta_1,\infty)$ 是 $\theta $ 的置信水平为 $1-\alpha$ 的单侧置信区间，$\theta_1$ 称为 $\theta$ 的置信水平为 $1-\alpha$ 的单侧置信下限.

若统计量 $\theta_2=\theta_2(X_1,X_2,\cdots,X_n)$，对于任意 $\theta\in\varTheta$ 满足：

$$
P(\theta<\theta_2)\geqslant 1-\alpha
$$

则称随机区间$(-\infty,\theta_2)$ 是 $\theta$ 的置信水平为 $1-\alpha$ 的单侧置信区间，$\theta_2$ 称为 $\theta$ 的置信水平为 $1-\alpha$ 的单侧置信上限.

正态分布总数学期望的单侧置信区间：

由前面定理，有($\mu$ 是待估参量)：

$$
\frac{\bar{X}-\mu}{\frac{S}{\sqrt{n}}}\sim t(n-1)
$$

于是有：

$$
P(\frac{\bar{X}-\mu}{\frac{S}{\sqrt{n}}}<t_{\alpha}(n-1))=1-\alpha
$$

于是有：

$$
P(\mu>\bar{X}-\frac{S}{\sqrt{n}}t_{\alpha}(n-1))=1-\alpha
$$

$\mu$ 的置信水平为 $1-\alpha$ 的单侧置信区间为：

$$
(\bar{X}-\frac{S}{\sqrt{n}}t_{\alpha}(n-1),+\infty)
$$

$\mu$ 的置信水平为 $1-\alpha$ 的单侧置信下限为：

$$
\mu_1
=\bar{X}-\frac{S}{\sqrt{n}}t_\alpha (n-1)
$$

又因为 

$$
\frac{(n-1)S^2}{\sigma^2}\sim \chi^2(n-1)
$$

于是有：

$$
P(\sigma^2<\frac{(n-1)S^2}{\chi^2_{1-\alpha}(n-1)})=1-\alpha
$$

于是 $\sigma^2$ 的置信水平为 $1-\alpha$ 的单侧置信区间为：

$$
(0,\frac{(n-1)S^2}{\chi^2_{1-\alpha}(n-1)})
$$

$\sigma^2$ 的置信水平为 $1-\alpha$ 的单侧置信上限为： 

$$
\sigma^2_2
=\frac{(n-1)S^2}{\chi^2_{1-\alpha}(n-1)}
$$

# 假设检验

在数理统计学上，“假设”是一个正确与否有待通过样本去判断的陈述

在数理统计学中，常用“检验”替代上文的“判断”

“认为假设正确”在统计上称为接受该假设

“认为假设不正确”在统计上称为否定或拒绝该假设

$$
P(当H_0为真时拒绝H_0)\leqslant \alpha
$$

令：

$$
P(当H_0为真时拒绝H_0)
=P_{\mu_0}(|\frac{\bar{X}-\mu_0}{\frac{\sigma}{\sqrt{n}}} |\geqslant k)=\alpha
$$

当 $H_0$ 为真，$Z=\frac{\bar{X}-\mu_0}{\frac{\sigma}{\sqrt{n}}}\sim N(0,1)$

$$
k=z_{\frac{\alpha}{2}}
$$

若 $Z$ 的观察值 $z$ 满足：

$$
|z|=|\frac{\bar{x}-\mu_0}{\frac{\sigma}{\sqrt{n}}}\geqslant k |=z_{\frac{\alpha}{2}}
$$

则拒绝 $H_0$

若 $Z$ 的观察值 $z$ 满足：

$$
|z|
=|\frac{\bar{x}-\mu_0}{\frac{\sigma}{\sqrt{n}}} |
<k
=z_{\frac{\alpha}{2}}
$$

$\alpha$ 称为**显著性水平**

**检验统计量**：在检验一个假设时所用的统计量称为检验统计量

$H_0$ 称为**原假设**或**零假设**或**解消假设**

$H_1$ 称为**备择假设** 或**对立假设**

**接受域**：使原假设得到接受的那些样本$(X_1,X_2,\cdots,X_n)$ 所在的区域称为该检验的接受域

**拒绝域或否定域或临界域**：当检验统计量取某个区域 $C$ 中的时，我们拒绝原假设 $H_0,$ 则称区域 $C$ 为拒绝域

**临界点**：拒绝域的边界点称为临界点

**简单假设**：只含一个参数的假设

**复杂假设**：参数的数量在两个及以上的假设

**第一类错误**（弃真错误）：在假设 $H_0$ 为真的情况下，拒绝 $H_0$ 

**第二类错误**（取伪错误）：在假设 $H_0$ 为非真的情况下，接受 $H_0$

**显著性检验**：只对犯第一类错误的概率加以控制，而不考虑第二类错误的概率的检验，称为显著性检验

双边备择假设：形如 $H_0:\mu=\mu_0,H_1:\mu\ne \mu_0$ 的假设检验问题

右边检验：$H_0:\mu\leqslant \mu_0,H_1:\mu>\mu_0$

左边检验：$H_0:\mu\geqslant\mu_0,H_1:\mu<\mu_0$

单边检验：右边检验与左边检验统称为单边检验

假设检验的一般步骤：

（1）：根据实际问题要求，提出原假设 $H_0$ 及备择假设 $H_1$

（2）：给定显著性水平 $\alpha$ 以及样本容量 $n$

（3）：确定检验统计量以及拒绝域的形式

（4）：按 $P(当 H_0 为真时拒绝 H_0)\leqslant \alpha$ 求出拒绝域

（5）：取样，根据样本观察值作出决策，是接受 $H_0$ 还是接受 $H_1$

## 正态总体方差的检验

### 单个总体 $N(\mu,\sigma^2)$ 均值 $\mu$ 的检验

$Z$ 检验：$\sigma^2$ 已知，关于 $\mu$ 的检验

利用统计量：

$$
Z=\frac{\bar{X}-\mu_0}{\frac{\sigma}{\sqrt{n}}}
$$

来确定拒绝域

$t$ 检验：$\sigma^2$ 未知，关于 $\mu$ 的检验

检验问题：

$$
H_0:\mu= \mu_0,~~H_1:\mu\ne \mu_0
$$

构造：

$$
t=\frac{\bar{X}-\mu_0}{\frac{S}{\sqrt{n}}}
$$

$$
P(拒绝H_0|H_0为真)
=P(|\frac{\bar{X}-\mu_0 }{\frac{S}{\sqrt{n}}}|\geqslant k| \mu=\mu_0)= \alpha
$$

解得：

$$
k
=t_{\frac{\alpha}{2}}(n-1)
$$

拒绝域为：

$$
|t|
=|\frac{\bar{x}-\mu_0}{\frac{s}{\sqrt{n}}} |\geqslant t_{\frac{\alpha}{2}}(n-1)
$$

### 两个正态总体均值差的检验（$t$ 检验）

可用 $t$ 检验法检验具有相同方差的两正态总体均值差的假设

设 $X_1,X_2,\cdots,X_{n_1}$ 是来自正态总体 $N(\mu_1,\sigma^2)$ 的样本，$Y_1,Y_2,\cdots,Y_{n_2}$ 是来自正态总体 $N(\mu_2,\sigma^2)$ 的样本，且两样本独立，分别记它们的样本均值为 $\bar{X},\bar{Y} $，记样本方差为 $S_1^2,S_2^2$. 设 $\mu_1,\mu_2,\sigma^2$ 均为未知，现在要检验假设：

$$
H_0:\mu_1-\mu_2=\delta,~~~H_1:\mu_1-\mu_2\ne \delta
$$

其中，$\delta$ 为已知常数

为求出拒绝域，取显著性水平为 $\alpha$

构造检验统计量 $t$ 为：

$$
t
=\frac{(\bar{X}-\bar{Y})-\delta}{S_W\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}}
$$

其中，$S_W^2=\frac{(n_1-)S_1^2+(n_2-1)S_2^2}{n_1+n_2-2},S_W=\sqrt{S_W^2} $

当 $H_0$ 为真时，拒绝域的形式为：

$$
\bigg|t \bigg|\geqslant k
$$




### 基于成对数据的检验（$t$ 检验）

## 正态总体方差的假设检验

### 单个总体的情况

### 两个总体的情况

拒绝域具有如下形式：

$$
\frac{s_1^2}{s_2^2}\geqslant k
$$

常数 $k$ 的确定方法如下：

$$
P(H_0为真时拒绝H_0)
=P(\frac{S_1^2}{S_2^2}\geqslant k|\sigma_1^2\leqslant \sigma_2^2 )
\leqslant P(\frac{\frac{S_1^2}{S_2^2}}{\frac{\sigma_1^2}{\sigma_2^2}}\geqslant k)
$$

令：

$$
P(\frac{\frac{S_1^2}{S_2^2}}{\frac{\sigma_1^2}{\sigma_2^2}}\geqslant k)=\alpha
$$

得：

$$
k=F_\alpha(n_1-1,n_2-1)
$$

即检验问题的拒绝域为：

$$
F
=\frac{s_1^2}{s_2^2}\geqslant F_\alpha(n_1-1,n_2-1)
$$

上述检验法称为 $F$ 检验法

## 置信区间与假设检验之间的关系

设 $(\theta_1,\theta_2)$ 是参数 $\theta$ 的一个置信水平为 $1-\alpha$ 的置信区间，则对任意 $\theta\in\varTheta$，有：

$$
P(\theta_1<\theta<\theta_2)\geqslant 1-\alpha
$$

考虑显著性水平为 $\alpha$ 的双边检验：

$$
H_0:\theta=\theta_0,~~~H_1:\theta\ne \theta_0
$$

拒绝域为：

$$
\theta_0\leqslant \theta_1~~~或~~~\theta_0\geqslant \theta_2
$$

### 分布拟合检验

### p 值检验法

假设检验问题的 $p$ 值是由检验统计量的样本观察值得出的原假设可被拒绝的最小显著性水平












